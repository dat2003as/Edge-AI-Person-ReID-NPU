# vector_database.py - FIXED VERSION
import os
import pickle
import faiss
import numpy as np
import threading
import time
import logging
import copy
from collections import defaultdict
from typing import Optional, List, Tuple
import config

logger = logging.getLogger(__name__)

class VectorDatabase_Manager:
    """
    Qu·∫£n l√Ω c∆° s·ªü d·ªØ li·ªáu vector Faiss.
    - H·ªó tr·ª£ l∆∞u nhi·ªÅu vector cho m·ªôt ID.
    - S·ª≠ d·ª•ng c∆° ch·∫ø b·ªè phi·∫øu k·∫øt h·ª£p, ∆∞u ti√™n ID c√≥ ƒëi·ªÉm trung b√¨nh cao nh·∫•t
    - üî• H·ªñ TR·ª¢ DYNAMIC NAMESPACES + T·ª∞ ƒê·ªòNG T·∫†O DB M·ªöI
    """
    def __init__(self, index_dir="faiss_indexes"):
        self.index_dir = index_dir
        os.makedirs(self.index_dir, exist_ok=True)
        
        # ============================================================
        # üî• PH·∫¶N 1: KH·ªûI T·∫†O LOCKS, STRUCTURES & METADATA TR∆Ø·ªöC
        # ============================================================
        self.db_lock = threading.Lock()
        self.dimensions = {}
        self.indexes = {}
        self.id_maps = {}
        self.has_unsaved_changes = {}
        
        # ‚úÖ PH·∫¢I KH·ªûI T·∫†O METADATA TR∆Ø·ªöC KHI G·ªåI B·∫§T K·ª≤ H√ÄM SAVE N√ÄO
        self.metadata = self._load_metadata()
        
        # ============================================================
        # üî• PH·∫¶N 2: LOAD HO·∫∂C T·∫†O M·ªöI NAMESPACES
        # ============================================================
        existing_namespaces = self._discover_namespaces()
        
        if existing_namespaces:
            print(f"üìÇ Ph√°t hi·ªán {len(existing_namespaces)} namespaces: {existing_namespaces}")
            for ns in existing_namespaces:
                self._load_namespace(ns)
        else:
            print(f"üÜï Database m·ªõi t·∫°i '{self.index_dir}', ƒëang kh·ªüi t·∫°o default namespaces...")
            # H√†m n√†y g·ªçi save_all_databases(), gi·ªù ƒë√£ an to√†n v√¨ metadata ƒë√£ t·ªìn t·∫°i
            self._init_default_namespaces()
        
        # ============================================================
        # PH·∫¶N 3: C·∫¨P NH·∫¨T FLAGS
        # ============================================================
        for ns in self.indexes.keys():
            if ns not in self.has_unsaved_changes:
                self.has_unsaved_changes[ns] = False
        
        print(f"‚úÖ Kh·ªüi t·∫°o Faiss Vector DB Manager ({self.index_dir}) th√†nh c√¥ng.")
    
    def _discover_namespaces(self) -> List[str]:
        """
        üî• T·ª∞ ƒê·ªòNG PH√ÅT HI·ªÜN NAMESPACES T·ª™ FILES
        T√¨m t·∫•t c·∫£ .index files trong th∆∞ m·ª•c
        """
        namespaces = []
        
        if not os.path.exists(self.index_dir):
            return namespaces
        
        for filename in os.listdir(self.index_dir):
            if filename.endswith('.index'):
                # L·∫•y t√™n namespace t·ª´ filename
                # V√≠ d·ª•: "face_features.index" ‚Üí "face_features"
                ns = filename[:-6]  # B·ªè ".index"
                namespaces.append(ns)
        
        return namespaces
    
    def _init_default_namespaces(self):
        """
        üî• KH·ªûI T·∫†O DEFAULT NAMESPACES KHI DB TR·ªêNG
        T·∫°o c√°c namespace c∆° b·∫£n t·ª´ config
        """
        # X√°c ƒë·ªãnh default namespaces d·ª±a v√†o index_dir
        if "cccd" in self.index_dir.lower():
            # Database CCCD: Ch·ªâ c·∫ßn Face namespace
            default_namespaces = {
                "CCCD_FACES": config.FACE_VECTOR_DIM
            }
            print("   üìã CCCD Database: T·∫°o namespace CCCD_FACES")
        else:
            # Database Tracking: C·∫ßn ReID + Face
            default_namespaces = {
                config.REID_NAMESPACE: config.OSNET_VECTOR_DIM,
                config.FACE_NAMESPACE: config.FACE_VECTOR_DIM
            }
            print(f"   üìã Tracking Database: T·∫°o namespaces {list(default_namespaces.keys())}")
        
        # T·∫°o c√°c namespace
        self.dimensions = default_namespaces
        
        for ns in self.dimensions:
            self.indexes[ns] = self._create_new_index(ns)
            self.id_maps[ns] = []
            print(f"      ‚úÖ Kh·ªüi t·∫°o '{ns}' (dim={self.dimensions[ns]})")
        
        # üî• L∆ØU NGAY L·∫¨P T·ª®C ƒê·ªÇ T·∫†O FILES
        print("   üíæ L∆∞u database m·ªõi...")
        self.save_all_databases()
    
    def _load_namespace(self, namespace: str):
        """
        üî• LOAD 1 NAMESPACE T·ª™ FILES
        T·ª± ƒë·ªông detect dimension t·ª´ index file
        """
        index_path, id_map_path = self._get_paths(namespace)
        
        # Load index
        if os.path.exists(index_path):
            index = faiss.read_index(index_path)
            self.indexes[namespace] = index
            self.dimensions[namespace] = index.d  # L·∫•y dimension t·ª´ index
            print(f"   ‚úÖ Loaded '{namespace}': {index.ntotal} vectors, dim={index.d}")
        else:
            print(f"   ‚ö†Ô∏è Index file not found: {index_path}")
            return
        
        # Load id_map
        if os.path.exists(id_map_path) and os.path.getsize(id_map_path) > 0:
            try:
                with open(id_map_path, 'rb') as f:
                    self.id_maps[namespace] = pickle.load(f)
            except (EOFError, pickle.UnpicklingError):
                print(f"‚ö†Ô∏è ID map '{id_map_path}' l·ªói. Kh·ªüi t·∫°o danh s√°ch tr·ªëng.")
                self.id_maps[namespace] = []
        else:
            print(f" ¬† ‚ö†Ô∏è ID map not found ho·∫∑c r·ªóng: {id_map_path}, kh·ªüi t·∫°o m·ªõi")
            self.id_maps[namespace] = []
    
    def _create_new_index(self, namespace: str) -> faiss.Index:
        """T·∫°o index m·ªõi cho namespace"""
        if namespace not in self.dimensions:
            raise ValueError(f"Dimension not defined for namespace '{namespace}'")
        
        dim = self.dimensions[namespace]
        return faiss.IndexFlatIP(dim)
    
    def _get_paths(self, namespace: str) -> Tuple[str, str]:
        """L·∫•y ƒë∆∞·ªùng d·∫´n index v√† id_map"""
        index_path = os.path.join(self.index_dir, f"{namespace}.index")
        id_map_path = os.path.join(self.index_dir, f"{namespace}.pkl")
        return index_path, id_map_path
    
    def _get_metadata_path(self):
        return os.path.join(self.index_dir, "metadata.pkl")
    
    def _load_metadata(self):
        path = self._get_metadata_path()
        # Ki·ªÉm tra file t·ªìn t·∫°i v√† c√≥ dung l∆∞·ª£ng l·ªõn h∆°n 0
        if os.path.exists(path) and os.path.getsize(path) > 0:
            try:
                with open(path, 'rb') as f:
                    return pickle.load(f)
            except (EOFError, pickle.UnpicklingError):
                print(f"‚ö†Ô∏è Metadata file '{path}' b·ªã l·ªói ho·∫∑c r·ªóng. Kh·ªüi t·∫°o m·ªõi.")
                return {}
        return {}
    
    def _save_data(self, namespace: str):
        """L∆∞u 1 namespace"""
        with self.db_lock:
            index_path, id_map_path = self._get_paths(namespace)
            if self.indexes[namespace] and self.has_unsaved_changes.get(namespace, False):
                faiss.write_index(self.indexes[namespace], index_path)
                with open(id_map_path, 'wb') as f:
                    pickle.dump(self.id_maps[namespace], f)
                print(f"ƒê√£ l∆∞u index v√† ID map cho namespace '{namespace}'.")
                self.has_unsaved_changes[namespace] = False
    
    def save_all_databases(self):
        """L∆∞u t·∫•t c·∫£ namespaces"""
        print("üíæ [DB] ƒêang th·ª±c hi·ªán l∆∞u d·ªØ li·ªáu...")
        with self.db_lock:
            for ns in self.indexes.keys():
                index_path, id_map_path = self._get_paths(ns)
                if self.indexes[ns] is not None:
                    faiss.write_index(self.indexes[ns], index_path)
                    with open(id_map_path, 'wb') as f:
                        pickle.dump(self.id_maps[ns], f)
            
            # L∆∞u Metadata
            meta_path = self._get_metadata_path()
            with open(meta_path, 'wb') as f:
                pickle.dump(self.metadata, f)
        print("‚úÖ [DB] L∆∞u th√†nh c√¥ng!")
    
    def add_vectors(self, namespace: str, vector_id: str, vectors_data: List[list]):
        """Th√™m vectors cho m·ªôt ID"""
        if not vectors_data:
            return
        
        # üî• KI·ªÇM TRA NAMESPACE T·ªíN T·∫†I
        if namespace not in self.indexes:
            print(f"‚ö†Ô∏è Namespace '{namespace}' kh√¥ng t·ªìn t·∫°i. Namespaces hi·ªán c√≥: {list(self.indexes.keys())}")
            return
        
        with self.db_lock:
            vectors_np = np.array(vectors_data, dtype='float32')
            faiss.normalize_L2(vectors_np)
            self.indexes[namespace].add(vectors_np)
            self.id_maps[namespace].extend([vector_id] * len(vectors_data))
            self.has_unsaved_changes[namespace] = True
    
    def search_vector_with_voting(self, namespace: str, query_vector: list) -> Optional[Tuple[str, float]]:
        """T√¨m ki·∫øm vector v·ªõi voting mechanism"""
        # üî• KI·ªÇM TRA NAMESPACE T·ªíN T·∫†I
        if namespace not in self.indexes:
            print(f"‚ö†Ô∏è Search failed: Namespace '{namespace}' kh√¥ng t·ªìn t·∫°i")
            return None
        
        index = self.indexes[namespace]
        
        # Ki·ªÉm tra DB tr·ªëng
        if index.ntotal == 0:
            return None
        
        # L·∫•y c·∫•u h√¨nh threshold
        if namespace == config.FACE_NAMESPACE or namespace == "CCCD_FACES" or namespace == "face":
            sim_threshold = config.FACE_DB_SEARCH_SIMILARITY_THRESHOLD
            min_votes_cfg = config.FACE_MIN_VOTES_FOR_MATCH
        else:
            sim_threshold = config.REID_DB_SEARCH_SIMILARITY_THRESHOLD
            min_votes_cfg = config.REID_MIN_VOTES_FOR_MATCH
        
        # T√¨m ki·∫øm Top-K
        query_np = np.array([query_vector], dtype='float32')
        faiss.normalize_L2(query_np)
        distances, indices = index.search(query_np, config.SEARCH_TOP_K)
        
        print(f"\nüîç [DB-Search] Namespace: {namespace} (T·ªïng: {index.ntotal} vector)")
        
        # Thu th·∫≠p ·ª©ng vi√™n
        candidates_data = defaultdict(list)
        for i, idx in enumerate(indices[0]):
            if idx == -1:
                continue
            score = float(distances[0][i])
            match_id = self.id_maps[namespace][idx]
            
            if score >= sim_threshold:
                candidates_data[match_id].append(score)
            
            status = "‚úÖ" if score >= sim_threshold else "‚ùå"
            print(f"   - {status} ID: {match_id:<12} | Score: {score:.4f}")
        
        # üî• B·ªè phi·∫øu TH√çCH ·ª®NG d·ª±a tr√™n SCORE
        finalists = []
        for mid, scores in candidates_data.items():
            vectors_in_db = self.count_vectors_for_id(namespace, mid)
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            combined_score = (avg_score * 0.7) + (max_score * 0.3)
            
            # üî• DYNAMIC THRESHOLD: Score cao ‚Üí √≠t votes c·∫ßn thi·∫øt
            if max_score >= config.DYNAMIC_MATCH_VERY_HIGH_THRESHOLD:  # >= 0.85
                adaptive_min_votes = config.DYNAMIC_MATCH_VERY_HIGH_MIN_VOTES  # 1 vote
            elif max_score >= config.DYNAMIC_MATCH_HIGH_THRESHOLD:  # >= 0.75
                adaptive_min_votes = config.DYNAMIC_MATCH_HIGH_MIN_VOTES  # 2 votes
            else:  # < 0.75
                adaptive_min_votes = config.DYNAMIC_MATCH_LOW_MIN_VOTES  # 3 votes
            
            # Fallback n·∫øu DB kh√¥ng c√≥ ƒë·ªß vectors
            adaptive_min_votes = min(adaptive_min_votes, vectors_in_db)
            
            if len(scores) >= adaptive_min_votes:
                finalists.append({
                    'id': mid,
                    'score': combined_score,
                    'votes': len(scores),
                    'in_db': vectors_in_db,
                    'max_score': max_score
                })
        
        if not finalists:
            print("   => ‚ö†Ô∏è Kh√¥ng ·ª©ng vi√™n n√†o ƒë·∫°t ng∆∞·ª°ng Votes.")
            return None
        
        # S·∫Øp x·∫øp v√† tr·∫£ v·ªÅ winner
        finalists.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"   => üèÜ CHI·∫æN TH·∫ÆNG: {finalists[0]['id']} (Score: {finalists[0]['score']:.4f}, Votes: {finalists[0]['votes']})")
        return finalists[0]['id'], float(finalists[0]['score'])
    
    def count_vectors_for_id(self, namespace: str, vector_id: str) -> int:
        """ƒê·∫øm s·ªë vector c·ªßa m·ªôt ID"""
        with self.db_lock:
            if namespace not in self.id_maps:
                return 0
            return self.id_maps[namespace].count(vector_id)
    
    def count_total_vectors(self, namespace: str) -> int:
        """
        üî• M·ªöI: ƒê·∫øm t·ªïng s·ªë vectors trong namespace
        """
        with self.db_lock:
            if namespace not in self.indexes:
                return 0
            
            index = self.indexes[namespace]
            if index is None:
                return 0
            
            return index.ntotal
    
    def get_max_person_id(self) -> int:
        """T√¨m s·ªë ID l·ªõn nh·∫•t t·ª´ Person_X"""
        max_id = 0
        all_ids = set()
        
        for namespace in self.id_maps:
            all_ids.update(self.id_maps[namespace])
        
        for person_id in all_ids:
            if isinstance(person_id, str) and person_id.startswith("Person_"):
                try:
                    num = int(person_id.split('_')[1])
                    if num > max_id:
                        max_id = num
                except (ValueError, IndexError):
                    continue
        return max_id
    
    def update_metadata(self, person_id: str, attributes: dict):
        """C·∫≠p nh·∫≠t metadata"""
        with self.db_lock:
            if person_id not in self.metadata:
                self.metadata[person_id] = {}
            self.metadata[person_id].update(attributes)
            return True
    
    def save_cccd_metadata(self, person_id: str, cccd_info: dict):
        """
        L∆∞u th√¥ng tin CCCD ƒë·∫ßy ƒë·ªß v√†o metadata
        Args:
            person_id: ID c·ªßa ng∆∞·ªùi (Person_X)
            cccd_info: Dict ch·ª©a {name, age, gender, race, country, cccd_number}
        """
        with self.db_lock:
            if person_id not in self.metadata:
                self.metadata[person_id] = {}
            
            # L∆∞u CCCD info v·ªõi flag matched=true
            self.metadata[person_id].update({
                'cccd_matched': True,
                'cccd_name': cccd_info.get('name', 'Unknown'),
                'cccd_age': cccd_info.get('age', 'Unknown'),
                'cccd_gender': cccd_info.get('gender', 'Unknown'),
                'cccd_race': cccd_info.get('race', 'Unknown'),
                'cccd_country': cccd_info.get('country', 'Unknown'),
                'cccd_number': cccd_info.get('cccd_number', 'Unknown'),
                'cccd_confidence': cccd_info.get('confidence', 0.0),
                'cccd_timestamp': time.time()
            })
            
            logger.info(f"[SAVE CCCD] {person_id}: {self.metadata[person_id].get('cccd_name')}")
            return True
    
    def get_metadata(self, person_id: str) -> dict:
        """L·∫•y metadata"""
        with self.db_lock:
            return self.metadata.get(person_id, {})

    def save_metadata(self):
        """
        üî• L∆ØU METADATA-ONLY (Non-blocking I/O)
        Copy d·ªØ li·ªáu trong lock, sau ƒë√≥ ghi file ngo√†i lock ƒë·ªÉ tr√°nh lag UI
        """
        try:
            # 1. Snapshot data (Fast, RAM only)
            with self.db_lock:
                data_snapshot = copy.deepcopy(self.metadata)
            
            # 2. Write to disk (Slow, IO) - NO LOCK HERE
            meta_path = self._get_metadata_path()
            with open(meta_path, 'wb') as f:
                pickle.dump(data_snapshot, f)
            
            # logger.info("üíæ [DB] Metadata saved (Async-ish)")
            return True
        except Exception as e:
            logger.error(f"‚ùå [DB SAVE ERROR] {e}")
            return False