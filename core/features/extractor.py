# core/features/extractor.py
import torch
import torchreid
import numpy as np
import cv2
from PIL import Image
from torchvision import transforms
import os 
import logging

logger = logging.getLogger(__name__)

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
MODELS_DIR = os.path.join(project_root, 'models')

# OSNet (Re-ID)
OSNET_MODEL_PATH = os.path.join(MODELS_DIR, "osnet_ain_x1_0_msmt17_256x128_amsgrad_ep50_lr0_0015_coslr_b64_fb10.pth")
OSNET_INPUT_SIZE = (128, 256)

# MobileFaceNet (Face Recognition)
MOBILEFACENET_MODEL_PATH = os.path.join(MODELS_DIR, "mobilefacenet.pt") 
MOBILEFACENET_INPUT_SIZE = (112, 112)

class Analyzer:
    """
    Class chuy√™n tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng:
    - OSNet cho Re-ID (To√†n th√¢n)
    - MobileFaceNet cho Face Recognition (Khu√¥n m·∫∑t)
    """
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.profiler = None 
        
        print(f"‚úÖ Analyzer kh·ªüi ch·∫°y tr√™n: {self.device}")

        try:
            # 1. T·∫£i model Re-ID
            self._load_osnet_model()

            # 2. T·∫£i model MobileFaceNet
            self._load_mobilefacenet_model()

        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o Analyzer: {e}")
            raise

   
    def _load_osnet_model(self):
        """T·∫£i model OSNet."""
        print("ƒêang t·∫£i model OSNet...")
        self.osnet_model = torchreid.models.build_model(
            name='osnet_ain_x1_0', num_classes=1000, loss='softmax', pretrained=False
        )
        torchreid.utils.load_pretrained_weights(self.osnet_model, OSNET_MODEL_PATH)
        self.osnet_model.to(self.device)
        self.osnet_model.eval()
        _, self.osnet_transform = torchreid.reid.data.transforms.build_transforms(
            height=OSNET_INPUT_SIZE[1], width=OSNET_INPUT_SIZE[0], is_train=False
        )

    def _load_mobilefacenet_model(self):
        """T·∫£i model MobileFaceNetV2."""
        print("ƒêang t·∫£i model MobileFaceNetV2...")
        if not os.path.exists(MOBILEFACENET_MODEL_PATH):
            raise FileNotFoundError(f"Thi·∫øu file: {MOBILEFACENET_MODEL_PATH}")

        self.face_model = torch.jit.load(MOBILEFACENET_MODEL_PATH, map_location=self.device)
        self.face_model.to(self.device)
        self.face_model.eval()
        print("‚úÖ T·∫£i MobileFaceNetV2 th√†nh c√¥ng.")

    def extract_reid_feature(self, person_crop: np.ndarray, body_mask: np.ndarray = None) -> list | None:
        """Tr√≠ch xu·∫•t vector ƒë·∫∑c tr∆∞ng Re-ID."""
        if person_crop is None or person_crop.size == 0:
            return None
        try:
            input_crop = person_crop
            if body_mask is not None:
                # Resize mask cho kh·ªõp ·∫£nh crop
                mask_resized = cv2.resize(body_mask, (person_crop.shape[1], person_crop.shape[0]))
                # Ch·ªâ gi·ªØ l·∫°i ph·∫ßn ng∆∞·ªùi (n·ªÅn th√†nh ƒëen tuy·ªát ƒë·ªëi)
                input_crop = cv2.bitwise_and(person_crop, person_crop, mask=mask_resized)

            rgb_crop = cv2.cvtColor(input_crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_crop)
            transformed_image = self.osnet_transform(pil_image).unsqueeze(0).to(self.device)

            with torch.no_grad():
                embedding = self.osnet_model(transformed_image)
            
            # Chu·∫©n h√≥a L2 cho Re-ID
            embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
            return embedding.cpu().numpy().flatten().tolist()
        except Exception as e:
            logger.error(f"L·ªói Re-ID: {e}")
            return None

        
    def extract_face_feature(self, face_crop: np.ndarray) -> tuple[list | None, float]:
        """Tr√≠ch xu·∫•t Face Vector b·∫±ng MobileFaceNet v·ªõi CLAHE & L2 Norm."""
        if face_crop is None or face_crop.size == 0:
            return None, 0.0
        
        if self.profiler: self.profiler.start("Face_MobileFaceNet")
        
        try:
            # 1. Ti·ªÅn x·ª≠ l√Ω CLAHE
            face_ready = face_crop
            # 2. Resize & Normalize chu·∫©n MobileFaceNet
            img_rgb = cv2.cvtColor(face_ready, cv2.COLOR_BGR2RGB)
            img_resized = cv2.resize(img_rgb, (112, 112))
            img_normalized = (img_resized.astype(np.float32) - 127.5) / 128.0
            
            # 3. Chuy·ªÉn tensor & ƒë∆∞a l√™n device
            img_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1)
            transformed_image = img_tensor.unsqueeze(0).to(self.device)

            with torch.no_grad():
                embedding = self.face_model(transformed_image)
            # 4. üî• CHU·∫®N H√ìA L2 (TƒÉng ƒëi·ªÉm s·ªë similarity)
            embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
            
            if self.profiler: self.profiler.stop("Face_MobileFaceNet")
            return embedding.cpu().numpy().flatten().tolist(), 1.0 

        except Exception as e:
            if self.profiler: self.profiler.stop("Face_MobileFaceNet")
            print(f"‚ùå L·ªói Face Feature: {e}")
            return None, 0.0