# core/attributes/face_processor.py
import cv2
import numpy as np
import logging
import mediapipe as mp
from skimage import transform as trans

logger = logging.getLogger(__name__)

class FaceProcessor:
    """
    Class ti·ªán √≠ch (Utility) chuy√™n x·ª≠ l√Ω h√¨nh ·∫£nh khu√¥n m·∫∑t v√† t√≠nh to√°n h√¨nh h·ªçc.
    Kh√¥ng ch·ª©a state c·ªßa model AI.
    """
    def __init__(self, detector=None):
        # S·ª≠ d·ª•ng instance CenterFace ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ ModelsHandler
        self.detector = detector
        self.mp_selfie = mp.solutions.selfie_segmentation.SelfieSegmentation(model_selection=1)
        
    def detect_face(self, image, threshold=0.4):
        """S·ª≠ d·ª•ng CenterFace ƒë·ªÉ t√¨m m·∫∑t v√† landmarks"""
        if self.detector is None:
            return None, None
        return self.detector.detect(image, threshold=threshold)
    
    MODEL_POINTS_5 = np.array([
        ( -225.0,  170.0, -135.0), # M·∫Øt tr√°i
        (  225.0,  170.0, -135.0), # M·∫Øt ph·∫£i
        (    0.0,    0.0,    0.0), # M≈©i
        ( -150.0, -150.0, -125.0), # Mi·ªáng tr√°i
        (  150.0, -150.0, -125.0)  # Mi·ªáng ph·∫£i
    ], dtype=np.float32)

    @staticmethod
    def check_image_quality(image, min_size=(64, 64), blur_threshold=25.0,dark_threshold=25):
        """
        Ki·ªÉm tra ch·∫•t l∆∞·ª£ng ·∫£nh m·∫∑t: ƒë·ªô n√©t v√† ƒë·ªô s√°ng.
        Tr·∫£ v·ªÅ False n·∫øu ·∫£nh qu√° m·ªù ho·∫∑c qu√° t·ªëi.
        """
        if image is None or image.size == 0: return False
        h, w = image.shape[:2]
        # 1. Check size
        if w < min_size[0] or h < min_size[1]: 
            return False
        
        # 2. Check blur (Laplacian Variance)
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            score = cv2.Laplacian(gray, cv2.CV_64F).var()
            if score < blur_threshold:
                return False # ·∫¢nh qu√° m·ªù
            # 2. --- üîπ KI·ªÇM TRA ƒê·ªò S√ÅNG (BRIGHTNESS) M·ªöI üîπ ---
            # T√≠nh ƒë·ªô s√°ng trung b√¨nh c·ªßa ·∫£nh x√°m
            mean_brightness = np.mean(gray)
            # N·∫øu ƒë·ªô s√°ng th·∫•p h∆°n ng∆∞·ª°ng (v√≠ d·ª• 50 tr√™n thang 255), coi l√† qu√° t·ªëi
            if mean_brightness < dark_threshold:
                # logger.warning(f"·∫¢nh qu√° t·ªëi (Brightness: {mean_brightness:.2f} < {dark_threshold}). B·ªè qua.")
                return False # Lo·∫°i b·ªè ngay l·∫≠p t·ª©c

        except:
            return False       
        return True
    @staticmethod
    def safe_crop(image, x1, y1, x2, y2, tag="Unknown"):
        if image is None or image.size == 0: return None
        h, w = image.shape[:2]
        x1, y1 = max(0, min(x1, w)), max(0, min(y1, h))
        x2, y2 = max(0, min(x2, w)), max(0, min(y2, h))
        if x2 <= x1 or y2 <= y1: return None
        return image[y1:y2, x1:x2]

    @staticmethod
    def get_face_with_margin(person_crop, face_bbox, margin_ratio=0.1):
        try:
            h, w = person_crop.shape[:2]
            fx1, fy1, fx2, fy2 = map(int, face_bbox)
            fw, fh = fx2 - fx1, fy2 - fy1
            
            side = max(fw, fh)
            margin = int(side * margin_ratio)
            
            cx, cy = (fx1 + fx2) // 2, (fy1 + fy2) // 2
            
            half_side = (side // 2) + margin
            nx1, ny1 = max(0, cx - half_side), max(0, cy - half_side)
            nx2, ny2 = min(w, cx + half_side), min(h, cy + half_side)
            
            face_img = person_crop[ny1:ny2, nx1:nx2]
            
            return face_img, (nx1, ny1) 
        except Exception as e:
            logger.error(f"L·ªói trong get_face_with_margin: {e}")
            return None, (0, 0)

    @staticmethod
    def check_face_straight_2d(lms):
        # lms shape (5, 2): 0:M·∫Øt tr√°i, 1:M·∫Øt ph·∫£i, 2:M≈©i
        try:
            # Ki·ªÉm tra Yaw (Xoay tr√°i/ph·∫£i)
            dist_l = np.linalg.norm(lms[2] - lms[0])
            dist_r = np.linalg.norm(lms[2] - lms[1])
            # T·ªâ l·ªá c√†ng l·ªõn -> M·∫∑t c√†ng nghi√™ng
            yaw_ratio = max(dist_l, dist_r) / (min(dist_l, dist_r) + 1e-6)
            
            # Ki·ªÉm tra Roll (Nghi√™ng ƒë·∫ßu vai)
            eye_diff_y = abs(lms[1][1] - lms[0][1])
            eye_dist = np.linalg.norm(lms[1] - lms[0])
            roll_ratio = eye_diff_y / (eye_dist + 1e-6)

            # Ng∆∞·ª°ng: yaw_ratio < 1.5 l√† kh√° th·∫≥ng. > 2.0 l√† nghi√™ng nhi·ªÅu.
            is_straight = yaw_ratio < 1.8 and roll_ratio < 0.2
            return is_straight, yaw_ratio
        except:
            return False, 99.0
        
    @staticmethod
    def align_face_2d(image, lms, output_size=112):
        """
        S·ª≠a ƒë·ªïi: Th√™m tham s·ªë output_size ƒë·ªÉ t√πy bi·∫øn 112 ho·∫∑c 224.
        """
        try:
            # T·ªça ƒë·ªô chu·∫©n cho 112x112
            base_dst = np.array([
                [38.2946, 51.6963], [73.5318, 51.5014], # M·∫Øt
                [56.0252, 71.7366],                     # M≈©i
                [41.5493, 92.3655], [70.7299, 92.2041]  # Mi·ªáng
            ], dtype=np.float32)

            # T·ªâ l·ªá h√≥a t·ªça ƒë·ªô chu·∫©n theo size m·ªõi
            ratio = output_size / 112.0
            dst_points = base_dst * ratio

            src_points = np.array(lms, dtype=np.float32)
            
            from skimage import transform as trans
            tform = trans.SimilarityTransform()
            tform.estimate(src_points, dst_points)
            M = tform.params[0:2, :]

            # Warp ·∫£nh theo size mong mu·ªën
            return cv2.warpAffine(image, M, (output_size, output_size), borderValue=0)
        except Exception as e:
            # Fallback n·∫øu l·ªói transform
            return cv2.resize(image, (output_size, output_size))

    @staticmethod
    def calculate_simple_golden_score(lms):
        try:
            # T·ªâ l·ªá 1: ƒê·ªô c√¢n ƒë·ªëi tr√°i ph·∫£i (M·∫Øt-M≈©i)
            dist_l = np.linalg.norm(lms[2] - lms[0])
            dist_r = np.linalg.norm(lms[2] - lms[1])
            balance_score = 100 - abs(dist_l - dist_r) / (dist_l + dist_r) * 100
            
            # T·ªâ l·ªá 2: Kho·∫£ng c√°ch m·∫Øt so v·ªõi ƒë·ªô r·ªông m·∫∑t (gi·∫£ ƒë·ªãnh qua 5 ƒëi·ªÉm)
            eye_dist = np.linalg.norm(lms[1] - lms[0])
            mouth_dist = np.linalg.norm(lms[4] - lms[3])
            # M·ªôt t·ªâ l·ªá khu√¥n m·∫∑t ƒë·∫πp th∆∞·ªùng c√≥ eye_dist / mouth_dist ~ 1.2
            ratio_val = eye_dist / (mouth_dist + 1e-6)
            ratio_score = 100 - abs(ratio_val - 1.2) * 50
            
            final_score = (balance_score + ratio_score) / 2
            return round(max(0, min(100, final_score)), 1)
        except:
            return 0
        
    @staticmethod
    def remove_background(face_img, bg_color=(128, 128, 128)):
        """
        Chuy·ªÉn background v√πng m·∫∑t v·ªÅ m√†u x√°m trung t√≠nh.
        """
        try:
            # Chuy·ªÉn BGR sang RGB cho MediaPipe
            img_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            results = FaceProcessor.mp_selfie.process(img_rgb)
            
            # T·∫°o m·∫∑t n·∫° (mask)
            condition = results.segmentation_mask > 0.5
            condition = np.stack((condition,) * 3, axis=-1)
            
            # T·∫°o n·ªÅn ph·∫≥ng
            bg_image = np.zeros(face_img.shape, dtype=np.uint8)
            bg_image[:] = bg_color
            
            # K·∫øt h·ª£p m·∫∑t v√† n·ªÅn m·ªõi
            output_image = np.where(condition, face_img, bg_image)
            return output_image
        except:
            return face_img
    @staticmethod
    def align_face_224(image, lms):
        """
        Align m·∫∑t v·ªÅ k√≠ch th∆∞·ªõc 224x224 s·ª≠ d·ª•ng 5 landmarks t·ª´ YuNet.
        
        Args:
            image: ·∫£nh g·ªëc (BGR, numpy array)
            lms: list ho·∫∑c array 10 ph·∫ßn t·ª≠ theo th·ª© t·ª± YuNet:
                [x_right_eye, y_right_eye, x_left_eye, y_left_eye,
                x_nose, y_nose, x_right_mouth, y_right_mouth,
                x_left_mouth, y_left_mouth]
        
        Returns:
            aligned_face: ·∫£nh ƒë√£ align 224x224 (ho·∫∑c None n·∫øu l·ªói)
        """
        try:
            # T·ªça ƒë·ªô chu·∫©n c·ªë ƒë·ªãnh cho 224x224 (t·ª´ InsightFace/ArcFace)
            dst_points = np.array([
                [76.5892, 103.3926],   # right eye
                [147.0636, 103.0028],  # left eye
                [112.0504, 143.4732],  # nose
                [83.0986, 184.7310],   # right mouth corner
                [141.4598, 184.4082]   # left mouth corner
            ], dtype=np.float32)

            # Chuy·ªÉn landmarks detect ƒë∆∞·ª£c th√†nh array 5x2
            src_points = np.array(lms, dtype=np.float32).reshape(5, 2)

            # ∆Ø·ªõc l∆∞·ª£ng similarity transform (xoay + scale + d·ªãch, gi·ªØ t·ª∑ l·ªá m·∫∑t t·ªët h∆°n)
            tform = trans.SimilarityTransform()
            tform.estimate(src_points, dst_points)
            M = tform.params[0:2, :]  # Ma tr·∫≠n 2x3

            # Warp ·∫£nh v·ªÅ ƒë√∫ng 224x224
            aligned = cv2.warpAffine(
                image, M, (224, 224),
                borderMode=cv2.BORDER_CONSTANT,
                borderValue=0,  # fill ƒëen v√πng ngo√†i
                flags=cv2.INTER_LINEAR
            )

            return aligned

        except Exception as e:
            print(f"Alignment failed: {e}")
            # Fallback: crop trung t√¢m v√† resize (n·∫øu landmarks l·ªói n·∫∑ng)
            h, w = image.shape[:2]
            size = min(h, w)
            cx, cy = w // 2, h // 2
            crop = image[cy - size//2 : cy + size//2, cx - size//2 : cx + size//2]
            return cv2.resize(crop, (224, 224))