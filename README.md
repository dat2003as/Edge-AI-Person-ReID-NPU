# Person Tracking & Re-Identification System

<div align="center">

**Há»‡ thá»‘ng tracking vÃ  nháº­n dáº¡ng ngÆ°á»i thÃ´ng minh cháº¡y trÃªn Orange Pi NPU**

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-red.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.5+-green.svg)
![YOLO](https://img.shields.io/badge/YOLO-11-yellow.svg)
![Orange Pi](https://img.shields.io/badge/Orange%20Pi-5%2F5%20Plus-orange.svg)
![Faiss](https://img.shields.io/badge/Faiss-Vector%20DB-orange.svg)

---

## ğŸ“¹ System Demos

<div align="center">

### ğŸŸ¢ Demo 1: Full System Performance
*Real-time tracking with Age, Gender, Clothing & CCCD Recognition*

https://github.com/user-attachments/assets/9cb270d4-4e60-46f7-9579-17256b7e09a6

---

### âš¡ Demo 2: Rapid Re-Identification
*Smart logic: Locking ID & Instant re-matching upon position change*

https://github.com/user-attachments/assets/ea34d5ae-1d0d-4140-a2ae-8fd0545b729a

</div>

---
![Status](https://img.shields.io/badge/Status-Production-success.svg)

*Advanced Re-ID + Face Recognition + Attributes Analysis + CCCD Integration*

</div>

---

## ğŸ“š Table of Contents

- [ğŸ† Technical Achievements](#-technical-achievements)
- [ğŸ¯ Key Features](#-key-features)
- [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“¦ Installation](#-installation)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ“Š Performance](#-performance)
- [ğŸ”§ Advanced Topics](#-advanced-topics)
- [ğŸ“– API Reference](#-api-reference)

---

## ğŸ† Technical Achievements

### 1. NPU Optimization with CIX Models âš¡

**Challenge**: KhÃ´ng cÃ³ tÃ i liá»‡u public vá» format CIX - SDK riÃªng cá»§a Orange Pi NPU

**Solution**: Reverse-engineered conversion pipeline tá»« proprietary SDK

**Results**:
- âœ… Converted 4 models: Gender (Face/Pose), Age/Race, Clothing
- âœ… **3-4x performance improvement** trÃªn NPU vs CPU (10-15 FPS vs 1-4 FPS)
- âœ… Äá»™ chÃ­nh xÃ¡c khÃ´ng thay Ä‘á»•i sau khi convert

**Models Deployed on NPU** (CIX Format - Proprietary):

| Model | File Pattern | Functionality | Input | Output |
|-------|--------------|---------------|-------|--------|
| **Gender (Face)** | `*.cix` | PhÃ¢n loáº¡i giá»›i tÃ­nh tá»« khuÃ´n máº·t<br/>Accuracy: >90% | Face crop (224x224) | Male/Female + confidence |
| **Gender (Pose)** | `*.cix` | PhÃ¢n loáº¡i giá»›i tÃ­nh tá»« tÆ° tháº¿ cÆ¡ thá»ƒ<br/>Fallback khi khÃ´ng detect Ä‘Æ°á»£c face | Body pose landmarks | Male/Female + confidence |
| **Age & Race** | `*.cix` | PhÃ¢n tÃ­ch Ä‘á»“ng thá»i tuá»•i vÃ  chá»§ng tá»™c<br/>Age Groups: 0-10, 11-19, 20-30...<br/>Race: Asian/White/Black/Indian/Other | Face crop (224x224) | Age Group + Race Class |
| **Clothing** | `*.cix` | PhÃ¢n loáº¡i trang phá»¥c<br/>Upper: Short/Long sleeve<br/>Lower: Shorts/Long pants/Skirt | Body crop with pose | Type + Color + Confidence |

**Models Deployed using ONNX Runtime / PyTorch**:

| Model | Framework | Functionality | Specifications |
|-------|-----------|---------------|----------------|
| **YOLO11 Nano** | ONNX | Person Detection & Tracking | 640x480, 5-7 FPS |
| **YuNet Face** | ONNX | Face Detection & Landmarks | Lightweight, Scale invariant |
| **OSNet AIN** | PyTorch | Re-ID Body Embedding (512D) | Robust to pose/clothing |
| **MobileFaceNet** | PyTorch | Face Embedding (128D) | 98% Accuracy (LFW) |

> **Note**: Model files khÃ´ng Ä‘Æ°á»£c upload lÃªn Git. Báº¡n cáº§n convert models tá»« ONNX/PyTorch sang CIX format báº±ng Orange Pi NOE SDK.

---

### 2. Advanced Re-ID System ğŸ¯

**Dual-Vector Matching Strategy vá»›i Background Removal**

#### Re-ID Pipeline (Body)

```
Input Frame â†’ Person Detection (YOLO11) 
    â†“
MediaPipe Selfie Segmentation (background removal)
    â†“
OSNet AIN x1.0 Embedding (512D vector)
    â†“
L2 Normalization
    â†“
Faiss Vector Search (Cosine Similarity)
```

**Technical Stack**:
- **Model**: OSNet AIN x1.0 (pretrained on MSMT17)
  - Input: 128x256 RGB
  - Output: 512-dimensional embedding vector
  - File: `osnet_ain_x1_0_msmt17_256x128_amsgrad_ep50_lr0_0015_coslr_b64_fb10.pth`

- **Background Removal**: MediaPipe Selfie Segmentation
  - Removes background noise before Re-ID extraction
  - Improves matching accuracy by 15-20%
  - Critical for cluttered environments

**Code Example**:
```python
# core/features/extractor.py
def extract_reid_feature(self, person_crop, body_mask=None):
    if body_mask is not None:
        # Resize mask to match crop
        mask_resized = cv2.resize(body_mask, (person_crop.shape[1], person_crop.shape[0]))
        # Remove background - only keep person
        input_crop = cv2.bitwise_and(person_crop, person_crop, mask=mask_resized)
    
    # OSNet transformation
    pil_image = Image.fromarray(cv2.cvtColor(input_crop, cv2.COLOR_BGR2RGB))
    transformed = self.osnet_transform(pil_image).unsqueeze(0).to(self.device)
    
    # Extract 512D embedding
    with torch.no_grad():
        embedding = self.osnet_model(transformed)
    
    # L2 normalization
    embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
    return embedding.cpu().numpy().flatten().tolist()
```

---

#### Face Recognition Pipeline

```
Input Frame (High-Res 2K recommended)
    â†“
YuNet Face Detection or CenterFace (CIX)
    â†“
Face Alignment & Crop (112x112)
    â†“
MobileFaceNet V2 Embedding (128D vector)
    â†“
L2 Normalization
    â†“
Faiss Vector Search
```

**Technical Stack**:
- **Model**: MobileFaceNet V2
  - Input: 112x112 RGB
  - Output: 128-dimensional embedding vector
  - Normalization: (img - 127.5) / 128.0
  - File: `mobilefacenet.pt` (TorchScript JIT)

**Code Example**:
```python
# core/features/extractor.py
def extract_face_feature(self, face_crop):
    # Preprocessing
    img_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
    img_resized = cv2.resize(img_rgb, (112, 112))
    img_normalized = (img_resized.astype(np.float32) - 127.5) / 128.0
    
    # Convert to tensor
    img_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1)
    transformed = img_tensor.unsqueeze(0).to(self.device)
    
    # Extract 128D embedding
    with torch.no_grad():
        embedding = self.face_model(transformed)
    
    # L2 normalization (critical for similarity)
    embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
    return embedding.cpu().numpy().flatten().tolist(), 1.0
```

---

#### Dynamic Threshold Matching

**Problem**: Standard thresholds fail trong mÃ´i trÆ°á»ng 3m (small space)

**Solution**: Adaptive voting vá»›i dynamic thresholds

```python
# config.py - Dynamic Matching Configuration
DYNAMIC_MATCH_VERY_HIGH_THRESHOLD = 0.85  # Score >= 0.85 â†’ Match ngay (1 vote)
DYNAMIC_MATCH_HIGH_THRESHOLD = 0.75       # 0.75 <= score < 0.85 â†’ Need 2 votes
DYNAMIC_MATCH_LOW_THRESHOLD = 0.75        # score < 0.75 â†’ Need 3 votes

# Confirmed Person Re-matching (3m scene)
CONFIRMED_FACE_SIMILARITY_THRESHOLD = 0.65  # Higher for face
CONFIRMED_REID_SIMILARITY_THRESHOLD = 0.55  # Lower for body (fallback)
TEMPORAL_MATCHING_WINDOW = 5                # Seconds
```

**Result**: 95%+ re-identification accuracy trong controlled environments

---

### 3. CCCD Integration vá»›i Priority Display ğŸ†”

**Feature**: Khi match CCCD, Æ°u tiÃªn hiá»ƒn thá»‹ thÃ´ng tin tá»« CCCD thay vÃ¬ AI attributes

#### CCCD Recognition Flow

```
High-Quality Face Crop (from 2K frame)
    â†“
MobileFaceNet V2 Embedding
    â†“
Search CCCD Database (Faiss namespace: "CCCD_FACES")
    â†“
Voting tá»« top 5 embeddings
    â†“
Confirmation Threshold: 0.58
    â†“
Save to DB vá»›i priority flag
    â†“
Display CCCD info (Name, Age, Gender, Country)
```

**Display Priority Logic**:
```python
# draw.py - Display Priority
if cccd_matched:
    name = cccd_metadata['name']          # From CCCD
    age = cccd_metadata['age']             # From CCCD  
    gender = cccd_metadata['gender']       # From CCCD
else:
    name = "Unknown"
    age = ai_attributes['age']             # From AI
    gender = ai_attributes['gender']       # From AI
```

**Key Features**:
- âœ… Name Voting System (tá»« multiple CCCD detections)
- âœ… Immutable Metadata (CCCD khÃ´ng bá»‹ ghi Ä‘Ã¨)
- âœ… Confidence-based matching vá»›i adaptive deduplication
- âœ… Metadata validation (check name != empty)

**Screenshots**:

![CCCD Detection Example 1](file:///C:/Users/09350/.gemini/antigravity/brain/3902d8eb-37c6-4b3a-aa6d-37fdc1f355cf/uploaded_image_0_1768731594875.png)

*VÃ­ dá»¥: System tracking "VÃµ Quá»‘c Äáº¡i" - CCCD matched, hiá»ƒn thá»‹ full attributes*

![CCCD Detection Example 2](file:///C:/Users/09350/.gemini/antigravity/brain/3902d8eb-37c6-4b3a-aa6d-37fdc1f355cf/uploaded_image_1_1768731594875.png)

*VÃ­ dá»¥: System tracking "Mai Khanh Huy" - Real-time attribute analysis*

---

### 4. 3-Worker Asynchronous Architecture âš™ï¸

**Problem**: Blocking I/O lÃ m cháº­m realtime processing
**Solution**: Queue-based multi-worker architecture phÃ¢n chia rÃµ rÃ ng theo luá»“ng xá»­ lÃ½

**Processing Flow Details**:

#### 1. MAIN THREAD: Camera + YOLO Detection
**Nhiá»‡m vá»¥:**
- Äá»c frame 2K (2560x1920) tá»« camera
- Resize vá» 640x480 cho YOLO
- PhÃ¡t hiá»‡n person bboxes
- PhÃ¢n phá»‘i task cho 3 workers qua Queue

#### 2. WORKER 1: Attribute Analysis
**Luá»“ng xá»­ lÃ½:**
```
ğŸ” Face Detection (YuNet)
    â†“
ğŸ“Š Gender Analysis (Face + Pose)
    â†“
ğŸ‘¤ Age + Race (ONNX)
    â†“
ğŸ˜Š Emotion Detection
    â†“
ğŸ‘• Clothing Analysis (Type + Color)
    â†“
ğŸ§¬ Vector Extraction (Face 128-dim + ReID 512-dim)
```

#### 3. WORKER 2: CCCD Recognition + LLM
**Luá»“ng xá»­ lÃ½:**
```
ğŸªª CCCD Detection & Recognition
    â†“
ğŸ“¤ SEND-1: AI Attributes â†’ LLM (Ngay khi Identified)
    â†“
ğŸ“¤ SEND-2: Periodic Update â†’ LLM (Má»—i 5 frames)
```

#### 4. WORKER 3: Result Processor + Consolidation
**Luá»“ng xá»­ lÃ½:**
```
ğŸ’¾ LÆ°u vectors vÃ o RAM (deque 30 frames gáº§n nháº¥t)
    â†“
ğŸ”„ CONSOLIDATE: Tá»•ng há»£p attributes tá»« history
    â†“
âœ… Identify/Confirm Check (theo ngÆ°á»¡ng Quality)
    â†“
ğŸ” Search Database (náº¿u Ä‘á»§ Ä‘iá»u kiá»‡n)
    â†“
ğŸ’¾ Update Metadata & Display
```

```mermaid
graph TB
    subgraph "Main Process"
        Camera[ZMQ Camera<br/>2K Res] --> FrameController[Frame Controller]
        FrameController --> YOLO[YOLO Person Detection<br/>640x480]
        YOLO --> TrackManager[Track Manager]
    end
    
    subgraph "Worker 1: AI Attributes"
        TrackManager --> |Queue 1|AIWorker[AI Worker]
        AIWorker --> Gender[Gender Hybrid]
        AIWorker --> AgeRace[Age & Race]
        AIWorker --> Clothing[Clothing & Color]
        AIWorker --> Vectors[Face & ReID Vectors]
    end
    
    subgraph "Worker 2: CCCD & LLM"
        TrackManager --> |Queue 2|CCCDWorker[CCCD Worker]
        CCCDWorker --> CCCDMatch[CCCD Recognition]
        CCCDWorker --> LLM[LLM Sender]
    end
    
    subgraph "Worker 3: Result Logic"
        AIWorker --> |Results|ResultWorker[Result Worker]
        CCCDWorker --> |Matches|ResultWorker
        ResultWorker --> Consolidation[Attribute Consolidation]
        Consolidation --> DB[Database & Metadata]
        ResultWorker --> Display[UI Rendering]
    end
    
    style Camera fill:#e1f5ff
    style YOLO fill:#fff4e1
    style AIWorker fill:#e8f5e9
    style CCCDWorker fill:#f3e5f5
    style ResultWorker fill:#fff3e0
```

**Queue Configuration**:
```python
# Queue sizes optimized for Orange Pi 5
AI_QUEUE_SIZE = 10       # AI attributes (gender, age, race, etc)
CCCD_QUEUE_SIZE = 5      # CCCD recognition (high-quality faces)
UI_QUEUE_SIZE = 10       # UI rendering
FRAME_QUEUE_SIZE = 100   # ZMQ frames buffer
```

**Benefits**:
- âœ… Non-blocking I/O
- âœ… Parallel processing
- âœ… Resource optimization
- âœ… 3x throughput improvement

---

### 5. Identification Lifecycle Stages ğŸ”„

Há»‡ thá»‘ng phÃ¢n chia tiáº¿n trÃ¬nh nháº­n dáº¡ng thÃ nh 4 giai Ä‘oáº¡n rÃµ rÃ ng:

| Giai Ä‘oáº¡n | Frames | MÃ u sáº¯c | Tráº¡ng thÃ¡i & HÃ nh Ä‘á»™ng |
|-----------|--------|---------|------------------------|
| ğŸ”µ **PENDING** | 0-10 | Xanh dÆ°Æ¡ng | **New Object detected**<br/>- GÃ¡n Track ID táº¡m (`Temp_X`)<br/>- Báº¯t Ä‘áº§u thu tháº­p vector (chÆ°a phÃ¢n tÃ­ch sÃ¢u) |
| ğŸŸ¡ **TENTATIVE** | 10-30 | Cam nháº¡t | **Analysing Attributes**<br/>- PhÃ¢n tÃ­ch Gender, Age, Race, Clothing<br/>- Consolidate attributes tá»« history (30 frames)<br/>- ChÆ°a search DB Ä‘á»ƒ tiáº¿t kiá»‡m tÃ i nguyÃªn |
| ğŸŸ  **IDENTIFIED** | > 30 | Cam Ä‘áº­m | **Database Matched / High Conf.**<br/>- TÃ¬m tháº¥y trong Vector DB<br/>- Hoáº·c AI Confidence > 0.7<br/>- **Action**: Gá»­i dá»¯ liá»‡u láº§n 1 cho LLM (SEND-1) |
| ğŸŸ¢ **CONFIRMED** | Locked | Xanh lÃ¡ | **Metadata Locked (IMMUTABLE)**<br/>- Attributes (Giá»›i tÃ­nh, tuá»•i, chá»§ng tá»™c) Ä‘Æ°á»£c khÃ³a láº¡i<br/>- Chá»‰ cáº­p nháº­t Emotion/Clothing/Last seen<br/>- Hiá»ƒn thá»‹ tÃªn (náº¿u cÃ³) hoáº·c Person ID chÃ­nh thá»©c |

---

### 6. Dual-Frame Processing Strategy ğŸ–¼ï¸

**Problem**: High-res needed for face quality, but slow to process full frame

**Solution**: Camera server vá»›i dual-resolution processing

#### Camera Architecture

```mermaid
graph LR
    subgraph "Camera Server (ZMQ Publisher)"
        CAM[IP Camera<br/>2K/1080p] --> FrameCapture[Frame Capture]
        FrameCapture --> HighRes[High-Res Frame<br/>1920x1080 or 2K]
    end
    
    subgraph "Tracking Client (Orange Pi)"
        HighRes --> |ZMQ Stream|Receiver[Frame Receiver]
        Receiver --> OriginalFrame[Original 2K Frame]
        OriginalFrame --> Resize[Resize to 640x480]
        
        Resize --> YOLO[YOLO Person Detection<br/>640x480]
        YOLO --> Tracking[BoT-SORT Tracking<br/>640x480 coords]
        
        Tracking --> |Scale coords|Mapper[Coordinate Mapper]
        OriginalFrame --> Mapper
        Mapper --> FaceExtract[Face Extract<br/>High-Res 2K]
        Mapper --> BodyExtract[Body Extract<br/>640x480]
        
        FaceExtract --> FaceRecog[MobileFaceNet<br/>Face Recognition]
        FaceExtract --> CCCD[CCCD Matching<br/>High Quality]
        BodyExtract --> ReID[OSNet Re-ID<br/>Body Embedding]
    end
    
    style CAM fill:#e1f5ff
    style HighRes fill:#fff4e1
    style YOLO fill:#e8f5e9
    style FaceExtract fill:#f3e5f5
    style CCCD fill:#fce4ec
```

#### Implementation Details

```python
# workers/app/main_loop.py - Main Processing Loop

# 1. Nháº­n frame tá»« Camera Server (ZMQ)
frame_original = zmq_cam.read()  # 2K resolution (1920x1080 or higher)

# 2. Resize xuá»‘ng 640x480 cho YOLO tracking (faster)
frame_resized = cv2.resize(frame_original, (640, 480))

# 3. YOLO detection + tracking trÃªn low-res frame
yolo_results = yolo_model.track(
    source=frame_resized,
    imgsz=640,        # YOLO input size
    conf=0.5,         # Confidence threshold
    iou=0.5,          # IoU for NMS
    classes=[0],      # Only person class
    tracker="botsort.yaml"
)

# 4. Láº¥y bounding boxes tá»« YOLO (640x480 coordinates)
for bbox_lowres in yolo_results:
    x1, y1, x2, y2 = bbox_lowres  # Coordinates in 640x480
    
    # 5. Scale coordinates vá» original 2K resolution
    scale_x = frame_original.shape[1] / 640  # e.g., 1920/640 = 3.0
    scale_y = frame_original.shape[0] / 480  # e.g., 1080/480 = 2.25
    
    x1_orig = int(x1 * scale_x)
    y1_orig = int(y1 * scale_y)
    x2_orig = int(x2 * scale_x)
    y2_orig = int(y2 * scale_y)
    
    # 6A. Extract BODY crop tá»« LOW-RES frame (cho Re-ID)
    body_crop_lowres = frame_resized[y1:y2, x1:x2]
    reid_vector = analyzer.extract_reid_feature(body_crop_lowres)
    
    # 6B. Extract FACE crop tá»« HIGH-RES frame (cho Face + CCCD)
    person_highres = frame_original[y1_orig:y2_orig, x1_orig:x2_orig]
    
    # Face detection trÃªn high-res crop
    faces = face_detector.detect(person_highres)
    
    for face_bbox in faces:
        fx1, fy1, fx2, fy2 = face_bbox
        face_crop_highres = person_highres[fy1:fy2, fx1:fx2]
        
        # 7. Face recognition vá»›i HIGH quality
        face_vector = analyzer.extract_face_feature(face_crop_highres)
        
        # 8. CCCD matching vá»›i HIGH quality
        cccd_result = cccd_recognizer.match(face_crop_highres)
```

#### Why This Approach?

| Task | Resolution | Reason |
|------|-----------|--------|
| **Person Detection** | 640x480 | YOLO nhanh hÆ¡n, Ä‘á»§ accuracy cho detection |
| **Body Re-ID** | 640x480 | OSNet robust vá»›i low-res, background removal quan trá»ng hÆ¡n resolution |
| **Face Recognition** | 2K/1080p | Face features cáº§n high detail, especially cho small faces |
| **CCCD Matching** | 2K/1080p | ID card photos cáº§n high quality Ä‘á»ƒ match chÃ­nh xÃ¡c |
| **Attributes (Gender/Age)** | 640x480 | Äá»§ resolution cho classification tasks |

#### Performance Benefits

âœ… **Speed**: YOLO tracking @ 5-7 FPS (trÃªn Orange Pi 5)

âœ… **Quality**: Face matching accuracy 98%+ (tá»« 2K frames)

âœ… **CCCD Accuracy**: 96%+ matching rate (high-res faces)

âœ… **Memory**: Chá»‰ resize khi cáº§n, khÃ´ng lÆ°u cáº£ 2 frames

âœ… **Scalability**: Camera server cÃ³ thá»ƒ stream multiple clients

âœ… **UI Display**: Follows YOLO tracking FPS (5-7 FPS)

#### Camera Server Configuration

```python
# utils/camera.py - ZMQ Camera Interface

class ZMQCamera:
    def __init__(self, ip="localhost", port=5555, high_res=True):
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.SUB)
        self.socket.connect(f"tcp://{ip}:{port}")
        self.socket.setsockopt_string(zmq.SUBSCRIBE, '')
        self.high_res = high_res  # Request high-res from server
    
    def read(self):
        """Receive frame from camera server"""
        image_bytes = self.socket.recv()
        np_array = np.frombuffer(image_bytes, dtype=np.uint8)
        frame = cv2.imdecode(np_array, cv2.IMREAD_COLOR)
        return frame  # Returns 2K/1080p frame
```

**Result**: **3x speed improvement** cho tracking + **High quality** cho face/CCCD

---

## ğŸ¯ Key Features

### Person Tracking Status Lifecycle

```
pending â†’ tentative â†’ identified â†’ confirmed
```

- **pending**: Má»›i phÃ¡t hiá»‡n, Ä‘ang thu tháº­p vectors
- **tentative**: Äá»§ vectors nhÆ°ng chÆ°a Ä‘áº¡t quality threshold
- **identified**: ÄÃ£ match vá»›i DB, chÆ°a fully stable
- **confirmed**: ÄÃ£ xÃ¡c nháº­n, lÆ°u vÃ o DB vÄ©nh viá»…n

### Attribute Analysis

**Immutable Attributes** (1-time lock khi confirmed):
- Gender, Age, Race
- First seen time
- CCCD information (if matched)

**Mutable Attributes** (real-time updates):
- Emotion
- Clothing (upper/lower type & color)
- Last seen time

### Focus Quality Gating

```python
# Chá»‰ confirm khi Ä‘á»§ cháº¥t lÆ°á»£ng
Face Sharpness: >= 120 (Laplacian variance)
Body Sharpness: >= 80
Face Size: >= 80x80 pixels
Body Area: >= 30,000 pixels
```

---

## ğŸ—ï¸ System Architecture

### Project Structure

```
track_khongLagTCP/
â”œâ”€â”€ main_new.py                 # ğŸ”¥ NEW ENTRY POINT
â”‚  
â”œâ”€â”€ core/                       # Core business logic
â”‚   â”œâ”€â”€ tracker/
â”‚   â”‚   â”œâ”€â”€ manager.py          # TrackManager - Main tracking logic
â”‚   â”‚   â”œâ”€â”€ matching.py         # ConfirmedPersonMatcher - Re-matching
â”‚   â”‚   â”œâ”€â”€ consolidation.py    # AttributeConsolidator - Voting
â”‚   â”‚   â””â”€â”€ utils.py            # TrackerUtils
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ extractor.py        # ğŸ”¥ Analyzer - OSNet + MobileFaceNet
â”‚   â”‚
â”‚   â”œâ”€â”€ attributes/
â”‚   â”‚   â”œâ”€â”€ manager.py          # AttributesManager
â”‚   â”‚   â”œâ”€â”€ face_processor.py   # FaceProcessor - Alignment
â”‚   â”‚   â””â”€â”€ models_handler.py   # AI Models Handler
â”‚   â”‚
â”‚   â”œâ”€â”€ cccd/
â”‚   â”‚   â””â”€â”€ recognition_new.py  # ğŸ”¥ CCCD Recognition + LLM Integration
â”‚   â”‚
â”‚   â”œâ”€â”€ collectors.py           # SmartFrameBuffer
â”‚   â””â”€â”€ frame_controller.py     # Frame Skip Controller
â”‚  
â”œâ”€â”€ workers/                    # Multi-worker architecture
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py             # Application entry point
â”‚   â”‚   â”œâ”€â”€ initializer.py      # System initialization
â”‚   â”‚   â”œâ”€â”€ main_loop.py        # Main processing loop
â”‚   â”‚   â”œâ”€â”€ ui_renderer.py      # UI rendering thread
â”‚   â”‚   â””â”€â”€ helpers.py          # Helper utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ ai_worker.py            # AI analysis worker
â”‚   â”œâ”€â”€ cccd_worker.py          # CCCD recognition worker
â”‚   â””â”€â”€ result_worker.py        # Result consolidation worker
â”‚
â”œâ”€â”€ utils/                      # Utility modules
â”‚   â”œâ”€â”€ age_race/
â”‚   â”‚   â”œâ”€â”€ age_race_cix.py     # Age & Race (CIX/NPU)
â”‚   â”‚   â””â”€â”€ age_race_onnx.py    # Age & Race (ONNX fallback)
â”‚   â”‚
â”‚   â”œâ”€â”€ gender/
â”‚   â”‚   â”œâ”€â”€ gender_cix.py       # Gender (CIX/NPU)
â”‚   â”‚   â””â”€â”€ gender_hybrid.py    # Gender Hybrid (Face + Pose)
â”‚   â”‚
â”‚   â”œâ”€â”€ clothing/
â”‚   â”‚   â”œâ”€â”€ clothing_new_cix.py # Clothing (CIX/NPU)
â”‚   â”‚   â”œâ”€â”€ pose_new.py         # Pose estimation
â”‚   â”‚   â””â”€â”€ pose_color_new1.py  # Color analysis from pose
â”‚   â”‚
â”‚   â”œâ”€â”€ detectors/
â”‚   â”‚   â”œâ”€â”€ yunet.py            # YuNet face detector
â”‚   â”‚   â”œâ”€â”€ cut_body_part.py    # Body part extraction
â”‚   â”‚   â””â”€â”€ mediapipe_pose.py   # MediaPipe pose
â”‚   â”‚
â”‚   â”œâ”€â”€ background_remover.py   # ğŸ”¥ MediaPipe background removal
â”‚   â”œâ”€â”€ emotion_detect.py       # Emotion detection
â”‚   â”œâ”€â”€ focus_quality_checker.py # Quality gating
â”‚   â”œâ”€â”€ camera.py               # ZMQ camera interface
â”‚   â”œâ”€â”€ NOE_Engine.py           # NPU engine wrapper
â”‚   â””â”€â”€ profiler.py             # Performance profiler
â”‚
â”œâ”€â”€ models/                     # AI Models (download separately)
â”œâ”€â”€ faiss_indexes/              # Vector database (tracking)
â”œâ”€â”€ faiss_indexes_cccd/         # Vector database (CCCD)
â”œâ”€â”€ config.py                   # ğŸ”¥ System configuration
â”œâ”€â”€ vector_database.py          # Faiss database manager
â”œâ”€â”€ draw.py                     # Visualization & UI
â””â”€â”€ tracked_object.py           # TrackedObject class
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Orange Pi 5/5 Plus** hoáº·c compatible SBC
- **Python 3.8+**
- **Ubuntu 20.04+** or Debian-based OS
- RAM: Min 4GB (recommended 8GB)

### Installation

```bash
# 1. Clone repository
cd /path/to/your/workspace
git clone <your-repo-url>
cd track_khongLagTCP

# 2. Create & activate virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download AI models (see Models section below)
# Place models in models/ directory

# 5. Create necessary directories
mkdir -p faiss_indexes faiss_indexes_cccd debug_aligned debug_faces debug_reid
```

### Run the System

```bash
# ğŸ”¥ NEW: Recommended entry point
python main_new.py
```

**Old entry points** (legacy, still functional):
```bash
python main.py                    # Single stream (simpler, easier to debug)
python main_dual_stream_bk.py     # Dual stream (deprecated, use main_new.py)
```

---

## ğŸ“¦ Models Download

### Required Models

Download vÃ  Ä‘áº·t vÃ o thÆ° má»¥c `models/`:

#### 1. Person Detection
- **`yolo11n.onnx`**
  - **MÃ´ hÃ¬nh**: YOLO11 Nano (phiÃªn báº£n Ultralytics má»›i nháº¥t)
  - **Chá»©c nÄƒng**: PhÃ¡t hiá»‡n ngÆ°á»i vÃ  tracking thá»i gian thá»±c
  - **Äáº§u vÃ o**: 640x480 hoáº·c 640x640 (resized frame)
  - **Äáº§u ra**: Bounding boxes cho lá»›p "person" (class_id=0)
  - **TÃ­ch há»£p**: BoT-SORT tracker Ä‘á»ƒ duy trÃ¬ ID tracking á»•n Ä‘á»‹nh
  - **Hiá»‡u nÄƒng**: 5-7 FPS trÃªn Orange Pi 5 (realtime tracking)
  - **Táº¡i sao chá»n Nano**: CÃ¢n báº±ng tá»‘t nháº¥t giá»¯a tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho thiáº¿t bá»‹ biÃªn

#### 2. Re-ID (Body Embedding)
- **`osnet_ain_x1_0_msmt17_256x128_amsgrad_ep50_lr0_0015_coslr_b64_fb10.pth`**
  - **MÃ´ hÃ¬nh**: OSNet AIN x1.0 (Omni-Scale Network vá»›i Instance Normalization)
  - **Chá»©c nÄƒng**: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng ngoáº¡i hÃ¬nh cÆ¡ thá»ƒ (body appearance)
  - **Pretrained**: MSMT17 dataset (1M+ áº£nh, 126K+ Ä‘á»‹nh danh)
  - **Äáº§u vÃ o**: 128x256 RGB person crop (Ä‘Ã£ tÃ¡ch ná»n)
  - **Äáº§u ra**: Vector embedding 512 chiá»u (L2-normalized)
  - **TÃ­nh nÄƒng chÃ­nh**: Chá»‘ng nhiá»…u do thay Ä‘á»•i tÆ° tháº¿ vÃ  quáº§n Ã¡o
  - **LÆ°u Ã½ quan trá»ng**: Báº¯t buá»™c dÃ¹ng kÃ¨m tÃ¡ch ná»n (MediaPipe) Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao

#### 3. Face Recognition
- **`mobilefacenet.pt`**
  - **MÃ´ hÃ¬nh**: MobileFaceNet V2 (TorchScript JIT)
  - **Chá»©c nÄƒng**: TrÃ­ch xuáº¥t face embedding 128D Ä‘á»ƒ so khá»›p khuÃ´n máº·t
  - **Äáº§u vÃ o**: 112x112 RGB face crop
  - **Äáº§u ra**: Vector 128 chiá»u (L2-normalized)
  - **Äá»™ chÃ­nh xÃ¡c**: 98%+ (khi káº¿t há»£p lá»c cháº¥t lÆ°á»£ng áº£nh)

- **`face_detection_yunet_2023mar.onnx`**
  - **MÃ´ hÃ¬nh**: YuNet (OpenCV DNN)
  - **Chá»©c nÄƒng**: PhÃ¡t hiá»‡n khuÃ´n máº·t vÃ  Ä‘iá»ƒm Ä‘áº·c trÆ°ng (facial landmarks)
  - **Äáº§u vÃ o**: Full frame (resize linh hoáº¡t, adaptive)
  - **Äáº§u ra**: Face bboxes + 5 landmarks (máº¯t, mÅ©i, miá»‡ng)
  - **TÃ­nh nÄƒng**: Scale invariant (detect tá»« 20px Ä‘áº¿n full frame), Orientation robust (xoay Â±45Â°)
  - **Vai trÃ²**: Face Detection chÃ­nh thá»©c cá»§a há»‡ thá»‘ng (Worker 1)

#### 4. Attributes Analysis (NPU Optimized)

**Gender Detection** (2 mÃ´ hÃ¬nh cho dual strategy):
- **`gender_face_model.cix`**
  - **Chá»©c nÄƒng**: PhÃ¢n loáº¡i giá»›i tÃ­nh tá»« khuÃ´n máº·t (phÆ°Æ¡ng phÃ¡p chÃ­nh)
  - **Kiáº¿n trÃºc**: ResNet-18 backbone
  - **Huáº¥n luyá»‡n**: Custom dataset vá»›i Ä‘á»™ chÃ­nh xÃ¡c validation 93%
  - **Dá»¯ liá»‡u**: 50K+ áº£nh máº·t (tá»‰ lá»‡ nam/ná»¯ cÃ¢n báº±ng)
  - **Äáº§u vÃ o**: Face crop 224x224
  - **Äáº§u ra**: Nam/Ná»¯ + Ä‘á»™ tin cáº­y (confidence score)
  - **Sá»­ dá»¥ng khi**: CÃ³ khuÃ´n máº·t rÃµ nÃ©t (confidence â‰¥ 0.6)

- **`gender_pose_model.cix`**
  - **Chá»©c nÄƒng**: PhÃ¢n loáº¡i giá»›i tÃ­nh tá»« dÃ¡ng ngÆ°á»i (phÆ°Æ¡ng phÃ¡p dá»± phÃ²ng)
  - **Kiáº¿n trÃºc**: Custom LSTM + CNN trÃªn MediaPipe landmarks
  - **Huáº¥n luyá»‡n**: Custom dataset vá»›i Ä‘á»™ chÃ­nh xÃ¡c validation 91%  
  - **Äáº§u vÃ o**: MediaPipe pose landmarks (33 Ä‘iá»ƒm keypoints)
  - **Äáº§u ra**: Nam/Ná»¯ + Ä‘á»™ tin cáº­y (confidence score)
  - **Sá»­ dá»¥ng khi**: Máº·t bá»‹ che, gÃ³c nghiÃªng, hoáº·c Ä‘á»™ tin cáº­y tháº¥p

**Age & Race Analysis**:
- **`age_race_combined.cix`**
  - **Chá»©c nÄƒng**: Há»c Ä‘a nhiá»‡m (Multi-task) cho cáº£ Tuá»•i vÃ  Chá»§ng tá»™c
  - **PhÃ¢n nhÃ³m tuá»•i**: `0-10`, `11-19`, `20-30`, `31-40`, `41-50`, `50-69`, `70+`
  - **Chá»§ng tá»™c**: `Asian` (Ã), `White` (Tráº¯ng), `Black` (Äen), `Indian` (áº¤n), `Others` (KhÃ¡c)
  - **Äáº§u vÃ o**: Face crop 224x224
  - **Äáº§u ra**: Index nhÃ³m tuá»•i + Index chá»§ng tá»™c
  - **Æ¯u Ä‘iá»ƒm**: Single inference, output mapped vá»›i labels há»‡ thá»‘ng

**Clothing Classification**:
- **`clothing_classifier.cix`**
  - **Chá»©c nÄƒng**: PhÃ¢n loáº¡i loáº¡i quáº§n Ã¡o + phÃ¢n tÃ­ch mÃ u sáº¯c
  - **PhÃ¢n loáº¡i**:
    - Top: Ão ngáº¯n tay, Ão dÃ i tay
    - Bot: Quáº§n ngáº¯n, Quáº§n dÃ i, VÃ¡y ngáº¯n, VÃ¡y dÃ i
  - **Äáº§u vÃ o**: Body crop + pose landmarks
  - **Äáº§u ra**: Class ID + Ä‘á»™ tin cáº­y
  - **Káº¿t há»£p**: Pose Estimation Ä‘á»ƒ xÃ¡c Ä‘á»‹nh vÃ¹ng Ã¡o/quáº§n Ä‘á»ƒ trÃ­ch xuáº¥t mÃ u sáº¯c

**Emotion Detection**:
- **`emotion_model.pt`** (PyTorch - chÆ°a convert NPU)
  - **Chá»©c nÄƒng**: PhÃ¡t hiá»‡n 7 cáº£m xÃºc cÆ¡ báº£n
  - **Cáº£m xÃºc**: Vui, Buá»“n, Giáº­n, Ngáº¡c nhiÃªn, BÃ¬nh thÆ°á»ng, Sá»£ hÃ£i, GhÃª tá»Ÿm
  - **Äáº§u vÃ o**: Face crop 224x224
  - **Äáº§u ra**: Lá»›p cáº£m xÃºc + Ä‘á»™ tin cáº­y
  - **Thá»i gian thá»±c**: Cáº­p nháº­t má»—i 0.5 giÃ¢y

#### 5. Pose Estimation & Utilities
- **`pose_landmarker.task`**
  - **MÃ´ hÃ¬nh**: MediaPipe Pose Landmarker (Google)
  - **Chá»©c nÄƒng**: TrÃ­ch xuáº¥t 33 Ä‘iá»ƒm má»‘c cÆ¡ thá»ƒ (tá»a Ä‘á»™ 3D)
  - **Äáº§u vÃ o**: Full person crop
  - **Äáº§u ra**: 33 keypoints (x, y, z, visibility)
  - **TrÆ°á»ng há»£p sá»­ dá»¥ng**:
    - PhÃ¢n loáº¡i giá»›i tÃ­nh tá»« dÃ¡ng ngÆ°á»i
    - PhÃ¢n vÃ¹ng quáº§n Ã¡o (thÃ¢n trÃªn/dÆ°á»›i)
    - TrÃ­ch xuáº¥t cÃ¡c bá»™ pháº­n cÆ¡ thá»ƒ
  - **Hiá»‡u nÄƒng**: Inference thá»i gian thá»±c

- **`skin_tone.csv`**
  - **Loáº¡i**: Dá»¯ liá»‡u tham chiáº¿u (Reference data)
  - **Chá»©c nÄƒng**: Báº£ng tra cá»©u phÃ¢n loáº¡i tÃ´ng da
  - **Ná»™i dung**: CÃ¡c dáº£i mÃ u RGB cho cÃ¡c tÃ´ng da khÃ¡c nhau
  - **Sá»­ dá»¥ng**: Tinh chá»‰nh nháº­n diá»‡n chá»§ng tá»™c káº¿t há»£p vá»›i AI model

---

## âš™ï¸ Configuration

### Key Configuration (config.py)

```python
# Tracking Thresholds
QUALITY_SCORE_THRESHOLD = 25            # NgÆ°á»¡ng kÃ­ch hoáº¡t nháº­n dáº¡ng
HIGH_CONF_FACE_SCORE = 10               # Äiá»ƒm cá»™ng khi face rÃµ nÃ©t
STABLE_IDENTIFICATION_THRESHOLD = 0.7   # NgÆ°á»¡ng confirmed
FACE_CONFIDENCE_THRESHOLD = 0.7         # NgÆ°á»¡ng tin cáº­y face

# Frame Processing
MAX_DISAPPEARED_FRAMES_BEFORE_DELETION = 50
ATTRIBUTE_ANALYSIS_INTERVAL = 5
YOLO_SKIP_FRAMES = 1                    # Skip frames for YOLO

# Vector Settings
REID_NAMESPACE = "reid_full_body"
FACE_NAMESPACE = "face_features"
OSNET_VECTOR_DIM = 512                  # OSNet embedding size
FACE_VECTOR_DIM = 128                   # MobileFaceNet embedding size

# Search & Voting
SEARCH_TOP_K = 15
FACE_DB_SEARCH_SIMILARITY_THRESHOLD = 0.55
REID_DB_SEARCH_SIMILARITY_THRESHOLD = 0.75
FACE_MIN_VOTES_FOR_MATCH = 2
REID_MIN_VOTES_FOR_MATCH = 5

# CCCD Configuration
CCCD_CONFIRMATION_THRESHOLD = 0.58      # CCCD matching threshold

# Confirmed Person Re-matching (3m scene)
TEMPORAL_MATCHING_WINDOW = 5
CONFIRMED_FACE_SIMILARITY_THRESHOLD = 0.65
CONFIRMED_REID_SIMILARITY_THRESHOLD = 0.55
```

---

## ğŸ“Š Performance

### Benchmarks (Orange Pi 5)

| Metric | Value |
|--------|-------|
| **YOLO Tracking FPS** | 5-7 FPS (Orange Pi 5) |
| **Overall System FPS** | ~7 FPS (1 person, 3-worker architecture) |
| **Attribute Models (CIX/NPU)** | 10-15 FPS (Gender, Age/Race, Clothing) |
| **Emotion Model (ONNX)** | 1-4 FPS (CPU inference) |
| **UI Display FPS** | 5-7 FPS (follows YOLO tracking) |
| **Re-ID Accuracy** | 95%+ (controlled environment) |
| **Face Match Accuracy** | 98%+ (with quality gating) |
| **CCCD Match Accuracy** | 96%+ (high-quality faces) |
| **NPU Utilization** | 60-80% (CIX models) |
| **Memory Usage** | ~2.5GB (with Faiss indexes) |
| **Latency** | \<100ms (per person detection) |

### Performance Optimization

```python
# Frame Skip - Process 1 out of every N frames
FRAME_SKIP_RATE = 3  # 30fps â†’ 10fps

# Queue Sizes
FRAME_QUEUE_MAX_SIZE = 100
AI_QUEUE_SIZE = 10
CCCD_QUEUE_SIZE = 5

# Memory Management
MOVING_AVERAGE_WINDOW = 10  # Sá»‘ vectors lÆ°u trong RAM
MAX_DISAPPEARED_FRAMES = 50  # Auto cleanup

# Disable debug images in production
SAVE_DEBUG_IMAGES = False
```

---

## ğŸ”§ Advanced Topics

### 1. LLM Integration (3-Stage System)

```python
# SEND-1: AI Attributes (when confidence >= 0.8)
dual_stream.send_stage1_ai_attributes(person_id, ai_attrs, llm_sender, obj_data)

# SEND-2: CCCD Match (when CCCD matched)
dual_stream.send_stage2_cccd_match(person_id, llm_sender, obj_data)

# SEND-3: Final Confirmed (when status='confirmed')
dual_stream.send_stage3_confirmed(person_id, ai_attrs, llm_sender, obj_data)
```

### 2. Custom Model Training

Äá»ƒ train custom models cho CIX conversion:

1. Train PyTorch/ONNX model
2. Sá»­ dá»¥ng CIX NOE Compiler (proprietary SDK)
3. Calibrate vá»›i .npy data tá»« training set
4. Build CIX model vá»›i build.cfg
5. Test trÃªn NPU

### 3. Camera Configuration

```python
# ZMQ Camera (recommended for network streaming)
ZMQ_IP = "localhost"
ZMQ_PORT = 5555

# USB Camera
CAM_INDEX = 0

# CSI Camera
CAM_INDEX = "/dev/video0"
```

---

## ğŸ“– API Reference

### TrackManager

```python
from core.tracker import TrackManager

track_manager = TrackManager(
    analyzer=reid_face_analyzer,
    db_manager=db_manager,
    dual_stream_manager=dual_stream,
    llm_sender=llm_sender
)

# Update tracking
track_manager.update_tracks(
    track_ids=track_ids,
    bboxes=bboxes,
    frame=frame_resized,
    attribute_task_queue=ai_queue,
    frame_original=frame_original,
    scale_x=scale_x,
    scale_y=scale_y
)
```

### Analyzer (Re-ID + Face)

```python
from core.features import Analyzer

analyzer = Analyzer()

# Extract Re-ID vector (512D)
reid_vector = analyzer.extract_reid_feature(person_crop, body_mask=mask)

# Extract Face vector (128D)
face_vector, confidence = analyzer.extract_face_feature(face_crop)
```

### VectorDatabase_Manager

```python
from vector_database import VectorDatabase_Manager

db_manager = VectorDatabase_Manager(index_dir="faiss_indexes")

# Add vectors
db_manager.add_vectors(
    namespace="face_features",
    person_id="P_001",
    vectors=[face_vector]
)

# Search
results = db_manager.search_vector_with_voting(
    namespace="face_features",
    query_vector=face_vector
)
```

### AttributesManager

```python
from core.attributes import AttributesManager

attr_manager = AttributesManager(
    reid_face_analyzer=analyzer,
    db_manager=db_manager
)

# Load models
await attr_manager.load_models()

# Analyze person
result = await attr_manager.analyze_person_by_bbox(
    frame_resized=frame_resized,
    frame_original=frame_original,
    bbox_resized=[x1, y1, x2, y2],
    bbox_original=[x1_orig, y1_orig, x2_orig, y2_orig],
    person_id=person_id,
    confirmed_status='pending'
)
```

---

## ğŸ› Troubleshooting

### Out of Memory

```python
# Reduce queue sizes
FRAME_QUEUE_MAX_SIZE = 50
AI_QUEUE_SIZE = 5
CCCD_QUEUE_SIZE = 3

# Reduce history window
MOVING_AVERAGE_WINDOW = 5
```

### Low FPS

```python
# Increase frame skip
FRAME_SKIP_RATE = 5

# Reduce analysis interval
ATTRIBUTE_ANALYSIS_INTERVAL = 10

# Disable debug
SAVE_DEBUG_IMAGES = False
```

### Camera Connection Failed

```bash
# Test ZMQ camera
python -c "from utils.camera import ZMQCamera; cam = ZMQCamera(); print(cam.read())"
```

---

## ğŸ“ License

Copyright Â© 2026 - All rights reserved

---

## ğŸ™ Acknowledgments

- **torchreid**: OSNet implementation
- **Ultralytics**: YOLO11
- **MediaPipe**: Pose estimation & Background removal
- **Faiss**: Vector similarity search
- **Orange Pi**: NPU SDK support

---

## ğŸ“§ Support

For technical support:
1. Check [Troubleshooting](#-troubleshooting) section
2. Review logs in terminal
3. Check debug folders: `debug_aligned/`, `debug_faces/`, `debug_reid/`

