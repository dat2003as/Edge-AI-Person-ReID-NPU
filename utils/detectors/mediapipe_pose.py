# utils/mediapipe_pose.py
import cv2
import numpy as np
import os
import time
import mediapipe as mp
from ultralytics import YOLO
from typing import Optional, List, Tuple, Dict

# Gi·∫£ s·ª≠ b·∫°n c√≥ m·ªôt module logging
from utils.logging_python_orangepi import get_logger
logger = get_logger(__name__)

mp_pose = mp.solutions.pose
# from mediapipe.tasks import python
# from mediapipe.tasks.python import vision
# from mediapipe.tasks.python.vision import RunningMode
        

# C√°c k·∫øt n·ªëi gi·ªØa c√°c ƒëi·ªÉm keypoint c·ªßa MediaPipe ƒë·ªÉ v·∫Ω skeleton
MEDIAPIPE_EDGES = [
    # Khu√¥n m·∫∑t
    (0, 1),   # M≈©i -> M·∫Øt trong tr√°i
    (1, 2),   # M·∫Øt trong tr√°i -> M·∫Øt tr√°i
    (2, 3),   # M·∫Øt tr√°i -> M·∫Øt ngo√†i tr√°i
    (3, 7),   # M·∫Øt ngo√†i tr√°i -> Tai tr√°i
    (0, 4),   # M≈©i -> M·∫Øt trong ph·∫£i
    (4, 5),   # M·∫Øt trong ph·∫£i -> M·∫Øt ph·∫£i
    (5, 6),   # M·∫Øt ph·∫£i -> M·∫Øt ngo√†i ph·∫£i
    (6, 8),   # M·∫Øt ngo√†i ph·∫£i -> Tai ph·∫£i
    (9, 10),  # Mi·ªáng tr√°i -> Mi·ªáng ph·∫£i

    # Th√¢n tr√™n
    (11, 12), # Vai tr√°i -> Vai ph·∫£i
    (11, 23), # Vai tr√°i -> H√¥ng tr√°i
    (12, 24), # Vai ph·∫£i -> H√¥ng ph·∫£i
    (23, 24), # H√¥ng tr√°i -> H√¥ng ph·∫£i

    # Tay tr√°i
    (11, 13), # Vai tr√°i -> Khu·ª∑u tay tr√°i
    (13, 15), # Khu·ª∑u tay tr√°i -> C·ªï tay tr√°i
    (15, 17), # C·ªï tay tr√°i -> Ng√≥n √∫t tr√°i (pinky)
    (15, 19), # C·ªï tay tr√°i -> Ng√≥n tr·ªè tr√°i (index)
    (15, 21), # C·ªï tay tr√°i -> Ng√≥n c√°i tr√°i
    (17, 19), # Ng√≥n √∫t tr√°i -> Ng√≥n tr·ªè tr√°i

    # Tay ph·∫£i
    (12, 14), # Vai ph·∫£i -> Khu·ª∑u tay ph·∫£i
    (14, 16), # Khu·ª∑u tay ph·∫£i -> C·ªï tay ph·∫£i
    (16, 18), # C·ªï tay ph·∫£i -> Ng√≥n √∫t ph·∫£i (pinky)
    (16, 20), # C·ªï tay ph·∫£i -> Ng√≥n tr·ªè ph·∫£i (index)
    (16, 22), # C·ªï tay ph·∫£i -> Ng√≥n c√°i ph·∫£i
    (18, 20), # Ng√≥n √∫t ph·∫£i -> Ng√≥n tr·ªè ph·∫£i

    # Ch√¢n tr√°i
    (23, 25), # H√¥ng tr√°i -> ƒê·∫ßu g·ªëi tr√°i
    (25, 27), # ƒê·∫ßu g·ªëi tr√°i -> M·∫Øt c√° tr√°i
    (27, 29), # M·∫Øt c√° tr√°i -> G√≥t ch√¢n tr√°i
    (27, 31), # M·∫Øt c√° tr√°i -> Ng√≥n ch√¢n c√°i tr√°i
    (29, 31), # G√≥t ch√¢n tr√°i -> Ng√≥n ch√¢n c√°i tr√°i

    # Ch√¢n ph·∫£i
    (24, 26), # H√¥ng ph·∫£i -> ƒê·∫ßu g·ªëi ph·∫£i
    (26, 28), # ƒê·∫ßu g·ªëi ph·∫£i -> M·∫Øt c√° ph·∫£i
    (28, 30), # M·∫Øt c√° ph·∫£i -> G√≥t ch√¢n ph·∫£i
    (28, 32), # M·∫Øt c√° ph·∫£i -> Ng√≥n ch√¢n c√°i ph·∫£i
    (30, 32)  # G√≥t ch√¢n ph·∫£i -> Ng√≥n ch√¢n c√°i ph·∫£i
]


class HumanDetection:
    """
    Class ƒë·ªÉ ph√°t hi·ªán ng∆∞·ªùi v√† ∆∞·ªõc t√≠nh t∆∞ th·∫ø b·∫±ng c√°ch k·∫øt h·ª£p YOLO v√† MediaPipe.
    C√°c h√†m ti·ªán √≠ch ƒë√£ ƒë∆∞·ª£c chuy·ªÉn th√†nh staticmethod ƒë·ªÉ d·ªÖ d√†ng t√°i s·ª≠ d·ª•ng.
    """
    def __init__(self, person_model='models/yolo11n.pt', pose_model='models/pose_landmarker.task'):
        logger.info('Init Human Detection with YOLO + MediaPipe Pose (Buffer Mode)')
        self.classes = [0]
        
        # 1. Kh·ªüi t·∫°o YOLO (Th∆∞ vi·ªán ultralytics x·ª≠ l√Ω file r·∫•t t·ªët)
        self.person_detector = YOLO(person_model)
        
        # --- üîπ GI·∫¢I PH√ÅP N·∫†P T·ª™ BUFFER (B·ªé QUA L·ªñI ƒê∆Ø·ªúNG D·∫™N WINDOWS) üîπ ---
        try:
            # T·ª± m·ªü file b·∫±ng Python (Python x·ª≠ l√Ω m·ªçi lo·∫°i ƒë∆∞·ªùng d·∫´n r·∫•t ·ªïn ƒë·ªãnh)
            if not os.path.exists(pose_model):
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file model t·∫°i: {pose_model}")
            
            with open(pose_model, 'rb') as f:
                model_buffer = f.read()
            
            # C·∫•u h√¨nh MediaPipe
            BaseOptions = mp.tasks.BaseOptions
            PoseLandmarker = mp.tasks.vision.PoseLandmarker
            PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions
            VisionRunningMode = mp.tasks.vision.RunningMode

            options = PoseLandmarkerOptions(
                # S·ª≠ d·ª•ng model_asset_buffer thay v√¨ model_asset_path
                base_options=BaseOptions(model_asset_buffer=model_buffer), 
                running_mode=VisionRunningMode.IMAGE,
                num_poses=1,
                output_segmentation_masks=True
            )
            
            # Kh·ªüi t·∫°o landmarker t·ª´ d·ªØ li·ªáu trong RAM
            self.landmarker = PoseLandmarker.create_from_options(options)
            logger.info("‚úÖ Pose Landmarker kh·ªüi t·∫°o th√†nh c√¥ng b·∫±ng ph∆∞∆°ng ph√°p Buffer.")

        except Exception as e:
            logger.error(f"‚ùå L·ªói n·∫°p model MediaPipe: {e}")
            self.landmarker = None
        
        self.fps_avg = 0.0
        self.call_count = 0
        self.last_results = None

    # def __init__(self, person_model: str = 'python/models/yolo11.rknn', pose_model: str = 'python/face_processing/models/pose_landmarker.task'):
    #     """
    #     Kh·ªüi t·∫°o l·ªõp HumanDetection.

    #     H√†m n√†y s·∫Ω t·∫£i model YOLO ƒë·ªÉ ph√°t hi·ªán ng∆∞·ªùi v√† model MediaPipe Pose Landmarker
    #     ƒë·ªÉ ph√°t hi·ªán c√°c ƒëi·ªÉm m·ªëc t∆∞ th·∫ø.

    #     Args:
    #         person_model (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model YOLO (.rknn).
    #         pose_model (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model MediaPipe Pose Landmarker (.task).
    #     """
    #     logger.info('Initializing Human Detection with a hybrid YOLO and MediaPipe Pose approach...')

    #     # --- 1. Kh·ªüi t·∫°o model YOLO ƒë·ªÉ ph√°t hi·ªán ng∆∞·ªùi ---
    #     self.classes = [0]  # L·ªõp 0 th∆∞·ªùng l√† 'person' trong COCO dataset
    #     self.person_detector = YOLO(person_model)
    #     logger.info(f"Person detector (YOLO) initialized successfully with model: {person_model}")

    #     # --- 2. Kh·ªüi t·∫°o MediaPipe Pose Landmarker ---
    #     # Ghi ch√∫: C√°c l·ªõp n√†y n√™n ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file ƒë·ªÉ code s·∫°ch h∆°n

    #     # T·∫°o c√°c t√πy ch·ªçn c·∫ßn thi·∫øt cho landmarker
    #     base_options = python.BaseOptions(model_asset_path=pose_model)
    #     options = vision.PoseLandmarkerOptions(
    #         base_options=base_options,
    #         running_mode=vision.RunningMode.IMAGE,
    #         num_poses=1,  # T·ªëi ∆∞u cho vi·ªác x·ª≠ l√Ω 1 ng∆∞·ªùi trong m·ªói ·∫£nh crop
    #         output_segmentation_masks=False
    #     )
        
    #     # T·∫°o landmarker t·ª´ c√°c t√πy ch·ªçn ƒë√£ ƒë·ªãnh nghƒ©a
    #     self.landmarker = vision.PoseLandmarker.create_from_options(options)
    #     logger.info(f"Pose Landmarker (MediaPipe) initialized successfully with model: {pose_model}")

    #     # --- 3. Kh·ªüi t·∫°o c√°c bi·∫øn theo d√µi hi·ªáu su·∫•t ---
    #     self.fps_avg = 0.0
    #     self.call_count = 0
    #     self.last_results = None
    def detect_pose_from_bbox(self, full_frame: np.ndarray, bbox: tuple):
            """
            ∆Ø·ªõc t√≠nh t∆∞ th·∫ø cho m·ªôt ng∆∞·ªùi duy nh·∫•t t·ª´ bounding box cho tr∆∞·ªõc.
            """
            try:
                # 1. Tr√≠ch xu·∫•t v√πng ·∫£nh c·ªßa ng∆∞·ªùi ƒë√≥ t·ª´ frame g·ªëc
                x1, y1, x2, y2 = map(int, bbox)
                padding = 10 # Th√™m m·ªôt ch√∫t ƒë·ªám ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t chi ti·∫øt
                person_crop_bgr = full_frame[max(0, y1-padding):y2+padding, max(0, x1-padding):x2+padding]

                if person_crop_bgr.size == 0:
                    return None, None

                # 2. Ch·∫°y MediaPipe Pose Landmarker tr√™n v√πng ·∫£nh ƒë√£ c·∫Øt
                person_crop_rgb = cv2.cvtColor(person_crop_bgr, cv2.COLOR_BGR2RGB)
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=person_crop_rgb)
                detection_result = self.landmarker.detect(mp_image)


                person_keypoints = None
                person_z = None
                body_mask = None
                # 3. Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô keypoints (Quan tr·ªçng!)
                # MediaPipe tr·∫£ v·ªÅ t·ªça ƒë·ªô t∆∞∆°ng ƒë·ªëi (0-1) so v·ªõi ·∫£nh crop.
                # Ch√∫ng ta c·∫ßn chuy·ªÉn n√≥ v·ªÅ t·ªça ƒë·ªô tuy·ªát ƒë·ªëi so v·ªõi frame g·ªëc.
                if detection_result.pose_landmarks:
                    pose_landmarks = detection_result.pose_landmarks[0]
                    crop_h, crop_w, _ = person_crop_rgb.shape
                    
                    person_keypoints = np.zeros((33, 3))
                    person_z = np.zeros(33)

                    for i, lm in enumerate(pose_landmarks):
                        # Chuy·ªÉn t·ª´ t·ªça ƒë·ªô t∆∞∆°ng ƒë·ªëi (trong crop) sang tuy·ªát ƒë·ªëi (trong crop)
                        local_x = lm.x * crop_w
                        local_y = lm.y * crop_h

                        # Chuy·ªÉn t·ª´ t·ªça ƒë·ªô tuy·ªát ƒë·ªëi (trong crop) sang tuy·ªát ƒë·ªëi (trong frame g·ªëc)
                        global_x = local_x + (x1 - padding)
                        global_y = local_y + (y1 - padding)

                        person_keypoints[i] = [global_x, global_y, lm.visibility]
                        person_z[i] = lm.z
                                    
                    if detection_result.segmentation_masks:
                        mask_data = detection_result.segmentation_masks[0].numpy_view()
                        # Chuy·ªÉn ƒë·ªïi sang mask 0-255 ƒë·ªÉ OpenCV d√πng ƒë∆∞·ª£c
                        body_mask = (mask_data > 0.5).astype(np.uint8) * 255
                    
                    return person_keypoints, person_z, body_mask
                
                return None, None,None # Kh√¥ng t√¨m th·∫•y pose
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω pose t·ª´ bbox: {e}")
                return None, None
    def run_detection(self, source: np.ndarray):
        start_time = time.time()
        image_rgb = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)
        
        # Ph√°t hi·ªán ng∆∞·ªùi b·∫±ng YOLO
        yolo_results = self.person_detector.predict(source=image_rgb, verbose=False, classes=self.classes, conf=0.5)
        logger.info(f"YOLO results: {len(yolo_results)} person detected")
        all_keypoints, all_z_values, boxes_data = [], [], []
        
        for box in yolo_results[0].boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
            conf = float(box.conf[0])
            boxes_data.append((x1, y1, x2, y2, conf))
            
            # Crop ·∫£nh ng∆∞·ªùi v·ªõi m·ªôt ch√∫t padding
            padding = 10
            person_crop_rgb = image_rgb[max(0, y1-padding):min(image_rgb.shape[0], y2+padding), max(0, x1-padding):min(image_rgb.shape[1], x2+padding)]
            
            if person_crop_rgb.shape[0] == 0 or person_crop_rgb.shape[1] == 0:
                continue

            # ∆Ø·ªõc t√≠nh t∆∞ th·∫ø tr√™n ·∫£nh ƒë√£ crop
            #mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=person_crop_rgb)
            person_crop_rgb = np.ascontiguousarray(person_crop_rgb)
            mp_image = mp.Image(mp.ImageFormat.SRGB,person_crop_rgb)
            detection_result = self.landmarker.detect(mp_image)
            
            # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô keypoint v·ªÅ h·ªá t·ªça ƒë·ªô c·ªßa ·∫£nh g·ªëc
            person_keypoints, person_z = np.zeros((33, 3)), np.zeros(33)
            if detection_result.pose_landmarks:
                pose_landmarks = detection_result.pose_landmarks[0]
                crop_h, crop_w, _ = person_crop_rgb.shape
                for i, lm in enumerate(pose_landmarks):
                    abs_x = (lm.x * crop_w) + (x1 - padding)
                    abs_y = (lm.y * crop_h) + (y1 - padding)
                    person_keypoints[i] = [abs_x, abs_y, lm.visibility]
                    person_z[i] = lm.z
                    
            all_keypoints.append(person_keypoints)
            all_z_values.append(person_z)

        # T√≠nh to√°n v√† ghi log FPS
        duration = time.time() - start_time
        fps_current = 1 / duration if duration > 0 else 0
        self.fps_avg = (self.fps_avg * self.call_count + fps_current) / (self.call_count + 1)
        self.call_count += 1
        logger.info(f"FPS Human detection (YOLO+MediaPipe): {self.fps_avg:.2f}")

        # Tr·∫£ v·ªÅ k·∫øt qu·∫£
        if not all_keypoints:
            self.last_results = None
            return np.array([]), [], np.array([])
            
        keypoints_data = np.array(all_keypoints)
        z_data = np.array(all_z_values)
        self.last_results = (keypoints_data, boxes_data)
        #logger.info(f"keypoints_data: {keypoints_data}")
        #logger.info(f"boxes_data: {boxes_data}")
        #logger.info(f"z_data: {z_data}")
        
        return keypoints_data, boxes_data, z_data

    # =====================================================================
    # ‚ú® C√ÅC H√ÄM TI·ªÜN √çCH ƒê√É ƒê∆Ø·ª¢C C·∫¨P NH·∫¨T TH√ÄNH STATICMETHOD
    # Gi·ªù ƒë√¢y ch√∫ng ta c√≥ th·ªÉ g·ªçi ch√∫ng t·ª´ b√™n ngo√†i: HumanDetection.select_best_arm(...)
    # =====================================================================
    # Th√™m h√†m n√†y v√†o class HumanDetection trong file mediapipe_pose.py

    @staticmethod
    def is_forearm_inside_torso(torso_poly: List[Tuple[int, int]], elbow: Dict, wrist: Dict) -> bool:
        """
        Ki·ªÉm tra xem ƒë∆∞·ªùng th·∫≥ng t·ª´ khu·ª∑u tay ƒë·∫øn c·ªï tay c√≥ n·∫±m b√™n trong ƒëa gi√°c th√¢n √°o kh√¥ng.
        S·ª≠ d·ª•ng cv2.pointPolygonTest ƒë·ªÉ hi·ªáu qu·∫£.
        """
        if not torso_poly or len(torso_poly) < 4 or elbow is None or wrist is None:
            return False

        try:
            # Chuy·ªÉn ƒë·ªïi ƒëa gi√°c th√†nh ƒë·ªãnh d·∫°ng numpy c·∫ßn thi·∫øt cho OpenCV
            contour = np.array(torso_poly, dtype=np.int32)
            
            # Ki·ªÉm tra c·∫£ hai ƒëi·ªÉm. Ch·ªâ c·∫ßn m·ªôt ƒëi·ªÉm n·∫±m trong ho·∫∑c tr√™n bi√™n l√† ƒë·ªß.
            # Gi√° tr·ªã > 0: trong, == 0: tr√™n bi√™n, < 0: ngo√†i
            is_elbow_in = cv2.pointPolygonTest(contour, (int(elbow['x']), int(elbow['y'])), False) >= 0
            is_wrist_in = cv2.pointPolygonTest(contour, (int(wrist['x']), int(wrist['y'])), False) >= 0
            
            # N·∫øu m·ªôt trong hai ƒëi·ªÉm n·∫±m trong, ta coi l√† c·∫≥ng tay "b√™n trong"
            return is_elbow_in or is_wrist_in
        except Exception:
            return False

    @staticmethod
    def _get_limb_coords(side, limb_type, keypoints, z_coords):
        """[STATICMETHOD] H√†m n·ªôi b·ªô ƒë·ªÉ l·∫•y t·ªça ƒë·ªô c·ªßa m·ªôt chi c·ª• th·ªÉ."""
        if limb_type == 'arm':
            indices = {
                'main': mp_pose.PoseLandmark[f'{side}_WRIST'].value,
                'p1': mp_pose.PoseLandmark[f'{side}_SHOULDER'].value,
                'p2': mp_pose.PoseLandmark[f'{side}_ELBOW'].value,
                'p3': mp_pose.PoseLandmark[f'{side}_WRIST'].value,
            }
            labels = ['shoulder', 'elbow', 'wrist']
        elif limb_type == 'leg':
            indices = {
                'main': mp_pose.PoseLandmark[f'{side}_ANKLE'].value,
                'p1': mp_pose.PoseLandmark[f'{side}_HIP'].value,
                'p2': mp_pose.PoseLandmark[f'{side}_KNEE'].value,
                'p3': mp_pose.PoseLandmark[f'{side}_ANKLE'].value,
            }
            labels = ['hip', 'knee', 'ankle']
        else:
            return None

        coords = {}
        for i, label in enumerate(labels):
            idx = indices[f'p{i+1}']
            x, y, vis = keypoints[idx]
            z = z_coords[idx]
            coords[label] = {'x': x, 'y': y, 'z': z, 'visibility': vis}
        return coords

    @staticmethod
    def select_best_arm(keypoints: np.ndarray, z_coords: np.ndarray, visibility_threshold: float = 0.9):
        """
        [STATICMETHOD] Ch·ªçn c√°nh tay t·ªët nh·∫•t d·ª±a tr√™n visibility v√† kho·∫£ng c√°ch Z (gi√° tr·ªã Z nh·ªè h∆°n l√† g·∫ßn camera h∆°n).
        """
        left_wrist_idx = mp_pose.PoseLandmark.LEFT_WRIST.value
        right_wrist_idx = mp_pose.PoseLandmark.RIGHT_WRIST.value
        
        left_vis = keypoints[left_wrist_idx][2]
        right_vis = keypoints[right_wrist_idx][2]
        left_z = z_coords[left_wrist_idx]
        right_z = z_coords[right_wrist_idx]

        left_valid = left_vis > visibility_threshold
        right_valid = right_vis > visibility_threshold

        best_side = None
        if left_valid and right_valid:
            # N·∫øu c·∫£ hai tay ƒë·ªÅu h·ª£p l·ªá, ch·ªçn tay c√≥ c·ªï tay g·∫ßn camera h∆°n
            best_side = 'LEFT' if left_z < right_z else 'RIGHT'
        elif left_valid:
            best_side = 'LEFT'
        elif right_valid:
            best_side = 'RIGHT'

        if best_side:
            # G·ªçi staticmethod kh√°c b·∫±ng t√™n Class
            coords = HumanDetection._get_limb_coords(best_side, 'arm', keypoints, z_coords)
            return best_side, coords
        
        return None, None

    @staticmethod
    def select_best_leg(keypoints: np.ndarray, z_coords: np.ndarray, visibility_threshold: float = 0.8):
        """
        [STATICMETHOD] Ch·ªçn ch√¢n t·ªët nh·∫•t d·ª±a tr√™n visibility v√† kho·∫£ng c√°ch Z.
        """
        left_ankle_idx = mp_pose.PoseLandmark.LEFT_ANKLE.value
        right_ankle_idx = mp_pose.PoseLandmark.RIGHT_ANKLE.value

        left_vis = keypoints[left_ankle_idx][2]
        right_vis = keypoints[right_ankle_idx][2]
        left_z = z_coords[left_ankle_idx]
        right_z = z_coords[right_ankle_idx]

        left_valid = left_vis > visibility_threshold
        right_valid = right_vis > visibility_threshold
        
        best_side = None
        if left_valid and right_valid:
            # N·∫øu c·∫£ hai ch√¢n ƒë·ªÅu h·ª£p l·ªá, ch·ªçn ch√¢n c√≥ m·∫Øt c√° g·∫ßn camera h∆°n
            best_side = 'LEFT' if left_z < right_z else 'RIGHT'
        elif left_valid:
            best_side = 'LEFT'
        elif right_valid:
            best_side = 'RIGHT'

        if best_side:
            # G·ªçi staticmethod kh√°c b·∫±ng t√™n Class
            coords = HumanDetection._get_limb_coords(best_side, 'leg', keypoints, z_coords)
            return best_side, coords

        return None, None

    @staticmethod
    def get_torso_box(keypoints: np.ndarray, visibility_threshold: float = 0.8):
        """
        [STATICMETHOD] T√≠nh to√°n bounding box cho ph·∫ßn th√¢n tr√™n (torso)
        d·ª±a tr√™n v·ªã tr√≠ c·ªßa vai v√† h√¥ng.
        """
        torso_indices = [
            mp_pose.PoseLandmark.LEFT_SHOULDER.value,
            mp_pose.PoseLandmark.RIGHT_SHOULDER.value,
            mp_pose.PoseLandmark.LEFT_HIP.value,
            mp_pose.PoseLandmark.RIGHT_HIP.value
        ]
        torso_points = []
        for idx in torso_indices:
            # ƒê·∫£m b·∫£o ch·ªâ s·ªë kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n c·ªßa m·∫£ng keypoints
            if idx < len(keypoints) and keypoints[idx][2] > visibility_threshold:
                torso_points.append(keypoints[idx][:2])
        
        # C·∫ßn √≠t nh·∫•t 3 ƒëi·ªÉm ƒë·ªÉ x√°c ƒë·ªãnh m·ªôt v√πng ƒë√°ng tin c·∫≠y
        if len(torso_points) < 3:
            return None

        torso_points = np.array(torso_points, dtype=np.int32)
        x1 = np.min(torso_points[:, 0])
        y1 = np.min(torso_points[:, 1])
        x2 = np.max(torso_points[:, 0])
        y2 = np.max(torso_points[:, 1])

        return x1, y1, x2, y2

    def draw_results(self, image: np.ndarray, min_conf: float = 0.5):
        """V·∫Ω k·∫øt qu·∫£ ph√°t hi·ªán (bounding box, skeleton) l√™n ·∫£nh."""
        if self.last_results is None:
            return image
            
        annotated_image = image.copy()
        keypoints_data, boxes_data = self.last_results
        
        for i, (kpts, box) in enumerate(zip(keypoints_data, boxes_data)):
            x1, y1, x2, y2, conf = box
            
            # V·∫Ω bounding box
            cv2.rectangle(annotated_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label = f"Person {i+1} ({conf:.2f})"
            cv2.putText(annotated_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # V·∫Ω c√°c ƒëi·ªÉm keypoint v√† n·ªëi ch√∫ng th√†nh skeleton
            points = {}
            for j, (x, y, visibility) in enumerate(kpts):
                if visibility > min_conf:
                    points[j] = (int(x), int(y))
                    cv2.circle(annotated_image, points[j], 3, (0, 0, 255), -1)
            
            for start_idx, end_idx in MEDIAPIPE_EDGES:
                if start_idx in points and end_idx in points:
                    cv2.line(annotated_image, points[start_idx], points[end_idx], (255, 255, 0), 1)
                    
        return annotated_image

# Kh·ªëi ch·∫°y ch√≠nh ƒë·ªÉ ki·ªÉm tra
if __name__ == "__main__":
    model_path = 'pose_landmarker_heavy.task'
    if not os.path.exists(model_path):
        print(f"Vui l√≤ng t·∫£i model '{model_path}' v√† ƒë·∫∑t v√†o c√πng th∆∞ m·ª•c.")
        exit()

    detector = HumanDetection(pose_model=model_path)
    
    video_path = "path/to/your/video.mp4" # << THAY ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N N√ÄY
    if not os.path.exists(video_path):
        print(f"Kh√¥ng t√¨m th·∫•y file video: {video_path}. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")
        exit()
        
    source = cv2.VideoCapture(video_path) 

    while True:
        ret, frame = source.read()
        if not ret:
            print("K·∫øt th√∫c video ho·∫∑c kh√¥ng th·ªÉ ƒë·ªçc frame.")
            break
        
        keypoints_data, boxes_data, z_data = detector.run_detection(frame)
        
        if len(keypoints_data) > 0:
            print("\n" + "="*40)
            
            # L·∫∑p qua t·ª´ng ng∆∞·ªùi ƒë·ªÉ t√¨m chi t·ªët nh·∫•t
            for i in range(len(boxes_data)):
                person_kpts = keypoints_data[i]
                person_z = z_data[i]
                box = boxes_data[i]
                x1, y1, _, _ = map(int, box[:4])
                
                print(f"--- Ng∆∞·ªùi {i+1} ---")
                
                # *** G·ªåI H√ÄM CH·ªåN TAY (d∆∞·ªõi d·∫°ng staticmethod) ***
                best_arm_side, arm_coords = HumanDetection.select_best_arm(person_kpts, person_z)
                if best_arm_side:
                    print(f"  üí™ C√°nh tay t·ªët nh·∫•t: {best_arm_side}")
                    cv2.putText(frame, f"ARM: {best_arm_side}", (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)

                # *** G·ªåI H√ÄM CH·ªåN CH√ÇN (d∆∞·ªõi d·∫°ng staticmethod) ***
                best_leg_side, leg_coords = HumanDetection.select_best_leg(person_kpts, person_z)
                if best_leg_side:
                    print(f"  ü¶µ Ch√¢n t·ªët nh·∫•t: {best_leg_side}")
                    cv2.putText(frame, f"LEG: {best_leg_side}", (x1, y1 - 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)


        annotated_frame = detector.draw_results(frame)
        cv2.imshow("Hybrid Pose Detection - Limb Selection", annotated_frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    source.release()
    cv2.destroyAllWindows()